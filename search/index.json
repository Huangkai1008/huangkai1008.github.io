[{"content":"HTTP/2 一、概述  HTTP/2 的主要目标是通过支持完整的请求与响应复用来减少延迟，通过有效压缩 HTTP 标头字段将协议开销降至最低，同时增加对请求优先级和服务器推送的支持。 为达成这些目标，HTTP/2 还给我们带来了大量其他协议层面的辅助实现，例如新的流控制、错误处理和升级机制。\n 为了实现 HTTP 工作组设定的性能目标，HTTP/2 引入了一个新的二进制分帧层，该层无法与之前的 HTTP/1.x 服务器和客户端向后兼容，因此协议的主版本提升到 HTTP/2。\n二、HTTP/1.x的缺陷 HTTP/1.x 实现简单是以牺牲性能为代价的：\n 客户端需要使用多个连接才能实现并发和缩短延迟 不会压缩请求和响应首部，从而导致不必要的网络流量 不支持有效的资源优先级，致使底层 TCP 连接的利用率低下  三、二进制分帧层 HTTP/2 所有性能增强的核心在于新的二进制分帧层（Binary framing layer），它定义了如何封装 HTTP 消息并在客户端与服务器之间传输。\n这里所谓的“层”，指的是位于套接字接口与应用可见的高级 HTTP API 之间一个经过优化的新编码机制: HTTP 的语义（包括各种动词、方法、标头）都不受影响，不同的是传输期间对它们的编码方式变了。 HTTP/1.x 协议以换行符作为纯文本的分隔符，而 HTTP/2 将所有传输的信息分割为更小的消息和帧，并采用二进制格式对它们编码。\n这样一来，客户端和服务器为了相互理解，都必须使用新的二进制编码机制: HTTP/1.x 客户端无法理解只支持 HTTP/2 的服务器，反之亦然。 现有的应用不必担心这些变化，因为客户端和服务器会替我们完成必要的分帧工作。\n数据流、消息和帧  数据流（Stream）：已建立的连接内的双向字节流，可以承载一条或多条消息。\n  消息（Message)：与逻辑请求或响应消息对应的完整的一系列帧\n  帧（Frame）：HTTP/2 通信的最小单位，每个帧都包含帧头，至少也会标识出当前帧所属的数据流\n 所有通信都在一个 TCP 连接上完成，此连接可以承载任意数量的双向数据流：\n 每个数据流都有一个唯一的标识符和可选的优先级信息，用于承载双向消息。 每条消息都是一条逻辑 HTTP 消息（例如请求或响应），包含一个或多个帧。 帧是最小的通信单位，承载着特定类型的数据，例如 HTTP 标头、消息负载等等。 来自不同数据流的帧可以交错发送，然后再根据每个帧头的数据流标识符重新组装。  HTTP/2 将 HTTP 协议通信分解为二进制编码帧的交换，这些帧对应着特定数据流中的消息。所有这些都在一个 TCP 连接内复用。\n四、请求与响应复用 在 HTTP/1.x 中，如果客户端要想发起多个并行请求以提升性能，则必须使用多个 TCP 连接。 这是 HTTP/1.x 交付模型的直接结果，该模型可以保证每个连接每次只交付一个响应（响应排队）。 更糟糕的是，这种模型也会导致队首阻塞，从而造成底层 TCP 连接的效率低下。\nHTTP/2 中新的二进制分帧层突破了这些限制，实现了完整的请求和响应复用: 客户端和服务器可以将 HTTP 消息分解为互不依赖的帧，然后交错发送，最后再在另一端把它们重新组装起来。\nHTTP/2 中的新二进制分帧层解决了 HTTP/1.x 中存在的队首阻塞问题，也消除了并行处理和发送请求及响应时对多个连接的依赖。 结果，应用速度更快、开发更简单、部署成本更低。\n五、服务器推送 HTTP/2 新增的另一个强大的新功能是，服务器可以对一个客户端请求发送多个响应。 换句话说，除了对最初请求的响应外，服务器还可以向客户端推送额外资源，而无需客户端明确地请求。\n六、标头压缩 HTTP/2 使用 HPACK 压缩格式压缩请求和响应标头元数据。\n参考资料  HTTP/2简介  ","date":"2020-07-10T15:51:57+08:00","permalink":"https://huangkai1008.github.io/p/http/2/","title":"HTTP/2"},{"content":"应用层 一、概述 应用层包括大多数应用程序使用的协议，用于通过较低级别的协议建立的网络连接提供用户服务或交换应用程序数据。\n网络应用程序体系结构 客户-服务器体系结构  存在总是打开的主机称为服务器（server），服务于其他来自客户（client）主机的请求，此种模式称为客户-服务器体系结构（client-server architecture）\n 特征  客户相互之间不直接通信 服务器具有固定的、周知的地址（IP 地址） 数据中心（data center） 用于扩展  示例  web 应用程序  P2P体系结构  以对等方式进行通信，并不区分客户端和服务端，而是平等关系进行通信。在对等方式下，可以把每个相连的主机当成既是服务端又是客户端，可以互相下载对方的共享文件。此种模式称为端到端体系结构（peer-to-peer architecture, P2P architecture）\n 特征  任意端系统直接通信 对等点从其他对等点请求服务，向其他对等点提供服务 自扩展性（self-scalability）：例如在 P2P 文件共享场景下，尽管每个对等方都由于请求文件产生工作负载，但每个对等方通过向其他对等方分发文件也为文件共享系统增加服务能力  示例  文件共享（BitTorrent） 对等方协助下载加速器（迅雷） 因特网电话和视频应用（Skype）  进程通信 在不同端系统上的进程（process），通过跨越计算机网络发送报文（message）\n客户与服务器进程  在一对进程之间的通信会话场景中，发起通信（即在该会话开始时发起与其他进程的联系）的进程被标识为客户，在会话开始时等待被联系的进程是服务器进程\n 对于 web 而言，浏览器是一个客户进程，Web 服务器是一个服务器进程；\n对于 P2P 文件共享，下载文件的对等方标识为客户，上载文件的对等方标识为服务器。\n进程与计算机网络间的接口 进程通过套接字向网络发送报文和从网络接收报文。\n 套接字（socket） 是同一台主机内应用层与运输层之间的接口。由于该套接字是建立网络应用的可编程接口，因此套接字也称为应用程序和网络之间的应用程序编程接口（Application Programming Interface, API）\n 应用程序开发者可以控制套接字在应用层端的一切，但是对该套接字的运输层端控制仅限于：\n 选择运输层协议 设定运输层一些参数，比如最大缓存和最大报文段长度等  应用程序建立在选择的运输层协议提供的运输层服务之上。\n进程寻址 为了标识接收进程，需要定义两种信息：\n 主机的地址（IP地址） 在目的主机中指定接收进程的标识符（端口号）  运输服务要求 可靠数据传输  如果一个协议确保由应用程序的一端发送的数据正确、完全地交付给该应用程序的另一端，那么就认为此协议提供了可靠数据传输（reliable data transfer）\n 当运输层协议不能提供可靠数据传输时，此协议可能能被容忍丢失的应用（loss-tolerant application） 所接受。\n吞吐量 运输层协议能够以某种特定的速率提供确保的可用吞吐量。使用此类服务，应用程序能够请求 r 比特/秒的确保吞吐量，并且该运输协议能够确保可用吞吐量总是为至少 r 比特/秒。\n具有吞吐量要求的应用程序被称为带宽敏感的应用（bandwidth-sensitive application），例如许多当前的多媒体应用；\n相反地，可以根据当时可用带宽利用可供使用吞吐量的应用被称为弹性应用（elastic application），例如电子邮件、文件传输应用。\n定时 安全性 二、应用层协议  应用层协议（application layer protocol） 定义了运行在不同端系统上的应用程序进程如何相互传递报文\n 具体定义内容为：\n 交换的报文类型 报文类型的语法 字段的语义 确定一个进程何时以及如何发送报文，对报文进行响应的规则  Web 与 HTTP 电子邮件系统 因特网电子邮件系统（email） 有三个主要组成部分：\n 用户代理(user agent) 邮件服务器（mail server） 简单邮件传输协议（Simple Mail Transfer Protocol, SMTP）  SMTP  SMTP 用于从发送方的邮件服务器发送报文到接收方的邮件服务器，SMTP 使用 TCP 作为它的支撑运输协议\n SMTP有两个部分：\n 运行在发送方邮件服务器的客户端 运行在接收方邮件服务器的服务端  每台邮件服务器同时运行 SMTP 的客户端也运行 SMTP 的服务端，根据邮件服务器的表现是发送/接收邮件决定它是 SMTP 的 客户端/服务端。\n邮件访问协议 与 HTTP 不同的是，HTTP是一个拉协议（pull protocol）， SMTP 是一个推协议(push protocol)，为了从邮件服务器上获取邮件到客户端，需要引入邮件访问协议，当前比较流行的邮件访问协议有：\n 第三版的邮局协议（Post Office Protocol--Version 3, POP3） 因特网邮件访问协议（Internet Mail Access Protocol, IMAP） 基于 Web 的电子邮件（HTTP）  DNS 协议  参考资料  Kurose, J. F., \u0026amp; Ross, K. W. (2018). 计算机网络-自顶而下方法 (7th ed.). 机械工业出版社.  ","date":"2020-06-25T22:20:45+08:00","permalink":"https://huangkai1008.github.io/p/%E5%BA%94%E7%94%A8%E5%B1%82/","title":"应用层"},{"content":"IP IP地址概述  **IP 地址(Internet Protocol address)**是用于识别 IP 网络中的设备的一个唯一地址，32 个二进制位分成了 4 个八位组（1 个八位组 = 8 个二进制位）。\n每个八位组转换成了十进制并由句点（点）分隔。 因此，可以说 IP 地址是用点分十进制格式表示的（例如，172.16.81.100）。 每个八位组值的范围从 0 到 255（十进制）或从 00000000 到 11111111（二进制）。\n 10. 1. 23. 19 (decimal) 00001010.00000001.00010111.00010011 (binary)  IP 地址编址方式 IP 地址的编址方式经历了三个历史阶段：\n 分类 子网划分 无分类  1. 分类  IP地址由两部分组成，网络号和主机号，其中不同分类具有不同的网络号长度，并且是固定的\n 网络号(net-id)：标志主机或路由器连接的网络，一个网络号在整个因特网内是唯一的\n  主机号(host-id)：标志该主机（或路由器）。一个主机号在它前面的网络号所指明的网络范围内必须是唯一的\n 简而言之，IP 地址 ::= {\u0026lt; 网络号 \u0026gt;, \u0026lt; 主机号 \u0026gt;}\n根据 IP 地址的范围，由此便划分出A、B、C三类及特殊地址D、E：\n   类别 起始位 开始 结束 点分十进制掩码     A 0 0.0.0.0 127.0.0.0 255.0.0.0   B 10 128.0.0.0 191.255.0.0 255.255.0.0   C 110 192.0.0.0 223.255.255.0 255.255.255.0    2. 子网划分  子网掩码  子网掩码(subnet mask)又叫网络掩码、地址掩码、子网络遮罩，它是一种用来指明一个IP地址的哪些位标识的是主机所在的子网，以及哪些位标识的是主机的位掩码。\n 通常情况下，子网掩码的表示方法和地址本身的表示方法是一样的。在IPv4中，就是点分十进制四组表示法（四个取值从0到255的数字由点隔开，比如255.128.0.0）或表示为一个八位十六进制数（如FF.80.00.00，它等同于255.128.0.0），后者用得较少。\n另一种更为简短的形式叫做无类别域间路由（CIDR）表示法，它给出的是一个地址加上一个斜杠以及网络掩码的二进制表示法中“1”的位数（即网络号中和网络掩码相关的是哪些位）。例如，192.0.2.96/28表示的是一个前28位被用作网络号的IP地址（和255.255.255.240的意思一样）。\n子网掩码是由32位二进制数字组成的四组数字，左边是网络位，用二进制数字1表示，1的个数等于网络位数的长度，右边是主机位，用二进制数字0表示，0的个数等于主机位的长度。\n当给定一个IP地址后，我们通过相应的子网掩码即可得出该地址所在网络的网络号位数，以此判断该网络能够容纳的机器的个数（即主机号位数）。另外的一个作用就是可以通过运算判断两台机器是否处在同一子网。\n特点\n 与IP地址一一对应 1和0永远是连续的，不会交叉出现 左边永远是1，右边永远是0  2.1 判断该网络能够容纳的机器的个数 ​ 主机号有N位，那么这个地址中，就只能有2 ** n − 2个主机，因为其中全0作为网络地址，全1作为广播地址\n2.2 判断两台机器是否处在同一子网（网段） ​ 将IP地址与子网掩码做与运算，如果得出的结果一样，则这两个IP地址是同一个子网当中\n3.广播地址(Broadcast address)  广播地址是专门用于同时向该网络中所有主机进行广播的一个地址。这就好像我们去收听一个广播频道，广播频道本身就是一个广播地址，播音员向这个地址去进行推送，那么只要能够收到这个频道的听众就都能够听到广播。那么这个广播的覆盖面到底有多广呢，这还是取决于我们的网络号。我们知道，一个完整的IP地址是由网络号和主机号两部分组成的，那么广播的覆盖范围就是其所在网络下的所有主机。\n 只要把主机号所在的二进制位全部变为1即可得到广播地址。\n 局域网地址：192.168.211.32/24（斜杠后的数字代表子网掩码的二进制位数，那么主机号的位数为32-24=8），所以广播地址为：192.168.211.255  ","date":"2020-06-18T12:22:57+08:00","permalink":"https://huangkai1008.github.io/p/ip/","title":"IP"},{"content":"计算机网络 一、概述 互联网、因特网和万维网  互联网(internet)：凡是由能彼此通信的设备组成的网络就叫互联网，互联网把多种不同的网络连接起来，因此互联网是网络的网络\n  因特网(Internet)：世界范围的计算机网络（computer network）\n  万维网(World Wide Web, WWW)：万维网是互联网的主要服务，提供网页和音视频等服务\n 关系 互联网（广义）\u0026gt; 因特网 \u0026gt; 万维网\n二、因特网 因特网包含了全世界数十亿计算设备，在今天，这些设备一般被称为主机（host） 或者端系统（end system），端系统通过通信链路（communication link） 和 分组交换机（packet switch） 连接到一起。\nISP  因特网服务提供商（Internet Service Provider, ISP），ISP 可以从互联网管理机构申请到很多 IP 地址，然后一些机构和个人从某个 ISP 获取 IP 地址的使用权，并可通过该 ISP 连接到互联网\n 例如中国移动、中国移动、中国电信就是有名的ISP\n协议  协议（protocol） 定义了在两个或者多个通信实体之间交换的报文的格式和顺序，以及报文发送和/或接收一条报文或其他事件所采取的动作\n 接入网 三、电路交换与分组交换 通过网络链路和交换机移动数据有两种基本方法：电路交换（circult switching）和分组交换（packet switching）\n电路交换（circuit switching）  电路交换需要建立一条专用的数据通信路径，这条路径上可能包含许多中间节点。这条通信路径在整个通信过程中将被独占，直到通信结束才会释放资源。电路交换适合实时性要求较高的大量数据传输的情况。\n电路交换最显著的特点：独占资源，最典型的电路交换：传统电话网络\n 电路交换中的多路复用（multiplexing）  物理链路的通信能力远远大于一路通信所需要的能力，可以通过多路复用提高信道利用率，同时各个通信线路之间又互不影响\n  频分多路复用（Frequency-Division Multiplexing, FDM） 时分多路复用（Time-Division Multiplexing, TDM） 码分多路复用（Code-Division Multiplexing, CDM） 波分多路复用（Wavelength-Division Multiplexing, WDM）  优势  通信延时小。通信双方通过专用线路进行通信，数据可以直达。当数据传输量较大时，优点将十分显著 线路独占，没有冲突 实时性强。一旦通信线路，建立，双方可以实时通信  劣势  线路独占，利用率太低 连接建立时间过长  分组交换（packet switching） 分组 在网络应用中，端系统彼此交换报文（message）。\n 为了从源端系统向目的端系统发送一个报文，源将长报文划分为小的数据块，这些小的数据块被称为分组（packet）\n 在源和目的地之间，每个分组都通过通信链路（communication link） 和 分组交换机（packet switch） 传送，分组以等于该链路最大传输速率的速度传输通过通信链路。\n存储转发传输 多数分组交换机在链路的输入端使用存储转发传输机制。\n 存储转发传输（store-and-forward transmission）：交换机能够向输出链路传输该分组的第一个比特前，必须接收到整个分组\n 优势  线路利用率更高 支持优先级传输 可靠性高 可以实现不同类型的数据终端设置  劣势  存在时延问题 分组必须携带一些控制信息需要额外的开销  节点时延 处理时延(nodal processing delay)  交换机、路由器等网络设备在收到报文后要进行解封装分析首部、提取数据、差错检验、路由选择等处理，此类时间被称为节点处理时延。一般来说，高速路由器的处理时延通常是微秒或者更低的数量级。\n 排队时延（queuing delay）  路由器或交换机等网络设备处理数据包排队所消耗的时间被称为排队时延。\n 排队时延的决定因素  R 链路的带宽(bps) L 数据包的大小(bits) a 数据包的平均到达时长  流量强度与排队时延  假定所有分组都是L比特组成，且队列无限大，则称 La/R 为流量强度 (traffic intensity)\n  La/R ~ 0：近乎为0 La/R -\u0026gt; 1：慢慢增大 La/R \u0026gt; 1：时延将会无穷大  因此：设计系统时流量强度不能大于1\n丢包（packet loss） 实际情况下输出队列容量是有限的，当分组到达时，队列是满的，路由器将会丢弃（drop） 该分组。\n传输时延（transmission delay）  路由器、交换机等网络设备将所有分组的比特推向链路所需要的时间被称为传输时延。\n 如果用 L 比特表示分组的长度，用 R bps(b/s) 表示从路由器A到路由器B的链路传输速率，传输时延是 L/R 。\n即： $$ d_{trans} = \\frac{L}{R} $$\n传播时延（propagation delay）  报文在实际的物理链路上传播数据所需要的时间被称为传播时延。\n 如果 d 是路由器 A 到路由器 B 之间的距离，s 是该链路之间的传播速率（传播速率取决于链路的物理媒体），传播时延是 d/s 。\n即： $$ d_{prop} = \\frac{d}{s} $$\n节点的总时延（total nodal delay） 节点的总时延 为处理时延、排队时延、传输时延和传播时延的和，即： $$ d_{nodal} = d_{proc} + d_{queue} + d_{trans} + d_{prop} $$\n端到端时延 通过由 N 条速率均为 R 的链路组成的路径，此时在源和目的地之间有 N - 1 台路由器，从源到目的地发送一个长度为L的分组，因为存储转发传输机制，假设网络此时是无拥塞的，那么端到端时延是：\n$$ d_{end-end} = N(d_{proc} + \\frac{L}{R} + d_{prop}) $$ 或: $$ d_{end-end} = N(d_{proc} + d_{trans} + d_{prop}) $$\nTraceroute  traceroute命令用于追踪数据包在网络上的传输时的全部路径\n traceroute -m 10 www.baidu.com traceroute to www.baidu.com (183.232.231.172), 10 hops max, 60 byte packets 1 _gateway (10.0.2.2) 0.321 ms 0.249 ms 0.106 ms 2 * * * 3 * * * 4 * * * 5 * * * 6 * * * 7 * * * 8 * * * 9 * * * 10 * * * 四、性能指标 互联网主要的性能指标有速率、带宽、吞吐量和时延等。\n速率  速率（传输速率，transmission rate） 是指计算机网络中的主机在数字信道上，单位时间内从一端传送到另一端的数据量，即数据传输率，也称数据率或比特率，单位为比特/秒（bit/s, bps）\n 不同的链路拥有不同的传输速率，一般讨论传输速率，往往指的是额定速率或标称速率（理想速率），此指标和物理媒体密切相关。\n带宽  带宽（bandwidth） 是指 计算机网络中的主机在数字信道上，单位时间内从一端传送到另一端的最大数据量，即最大速率\n 上行带宽与下行带宽  上行带宽是指用户电脑向网络发送信息时的数据传输速率，下行带宽是指网络向用户电脑发送信息时的传输速率\n 吞吐量  吞吐量（throughput） 是指单位时间内某个信道/端口实际的数据量，可以理解为实际的带宽\n 吞吐量等于瓶颈链路的传输速率，对于n条链路，链路速率分别为R1、R2 \u0026hellip; Rn，吞吐量为：min{R1, R2 \u0026hellip; Rn}\n瞬时吞吐量和平均吞吐量  从服务器到客户机通过计算机网络传送一个大文件，任意时刻客户机接收该文件的速率叫做瞬时吞吐量（instantaneous throughput），假设客户机接收该文件的所有 F 比特用了 T 秒，那么 F/T 就叫做平均吞吐量（average throughput）\n 五、 计算机网络体系结构 协议分层  网络设计者以 分层（layer） 的形式组织 协议（protocol） 以及实现这些协议的网络硬件和软件，各层的所有协议被称为协议栈（protocol stack）\n 优点  各层之间是独立的，某一层不需要知道其下层实现 灵活性好 结构上可分割开 易于实现和维护 能促进标准化工作  潜在缺点  一层可能冗余较低层的功能 某层功能可能需要仅在其他某层才出现的信息（如时间戳值），这违反了层次分离的目标  现在比较常见的一共有三种协议分层模型，分别为五层协议模型（因特网协议栈）、TCP/IP 模型、OSI 分层模型。\n因特网协议栈 因特网协议栈由**应用层（application layer)**、**运输层（transport layer）**、**网络层（internet layer）**、**链路层（link layer）**、**物理层 （physical layer）** 7 个层次组成。\nOSI 模型 OSI 模型由应用层（application layer)、表示层（presentation layer）、 会话层（session layer）、运输层（transport layer）、网络层（internet layer）、链路层（link layer）、物理层 （physical layer） 5 个层次组成。\nTCP/IP 模型 TCP/IP 模型由应用层（application layer)、运输层（transport layer）、网际层（internet layer）、链路层（link layer）、网络接口层 （network access layer） 5 个层次组成。\n参考资料  Kurose, J. F., \u0026amp; Ross, K. W. (2018). 计算机网络-自顶而下方法 (7th ed.). 机械工业出版社. Wikipedia : Internet_protocol_suite Wikipedia : OSI  ","date":"2020-06-17T11:22:54+08:00","permalink":"https://huangkai1008.github.io/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/","title":"计算机网络"},{"content":"DNS 和域名 一、域名 网域名称（英语：Domain Name，简称：Domain），简称域名、网域，是由一串用点分隔的字符组成的互联网上某一台计算机或计算机组的名称，用于在数据传输时标识计算机的电子方位。域名可以说是一个IP地址的代称，目的是为了便于记忆后者。\n域名的层级 www.example.com 真正的域名是 www.example.com.root ，简写为 www.example.com. 。因为，根域名 .root 对于所有域名都是一样的，所以平时是省略的。\n根域名的下一级，叫做顶级域名（top-level domain，缩写为TLD），比如 .com 、 .net ；再下一级叫做次级域名（second-level domain，缩写为SLD），比如 www.example.com 里面的 .example ，这一级域名是用户可以注册的；再下一级是主机名（host），比如www.example.com里面的www，又称为三级域名，这是用户在自己的域里面为服务器分配的名称，是用户可以任意分配的。\n 主机名.次级域名.顶级域名.根域名\nhost.sld.tld.root\n 二、DNS 域名系统（英语：Domain Name System，缩写：DNS）是一个分布式数据库，提供了域名和 IP地址之间相互转换的服务。\n查询过程 DNS 服务器根据域名的层级，进行分级查询。\n每一级域名都有自己的NS（Name Server） 记录，NS记录指向该级域名的域名服务器。这些服务器知道下一级域名的各种记录。\n \u0026ldquo;分级查询\u0026rdquo;，就是从根域名开始，依次查询每一级域名的NS记录，直到查到最终的IP地址\n过程大致为:\n 从\u0026quot;根域名服务器\u0026quot;查到\u0026quot;顶级域名服务器\u0026quot;的NS记录和A记录（IP地址） 从\u0026quot;顶级域名服务器\u0026quot;查到\u0026quot;次级域名服务器\u0026quot;的NS记录和A记录（IP地址） 从\u0026quot;次级域名服务器\u0026quot;查出\u0026quot;主机名\u0026quot;的IP地址   dig math.stackexchange.com ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.11.3-1ubuntu1.15-Ubuntu \u0026lt;\u0026lt;\u0026gt;\u0026gt; math.stackexchange.com ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 50719 ;; flags: qr rd ra; QUERY: 1, ANSWER: 4, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 65494 ;; QUESTION SECTION: ;math.stackexchange.com. IN A ;; ANSWER SECTION: math.stackexchange.com. 600 IN A 151.101.193.69 math.stackexchange.com. 600 IN A 151.101.129.69 math.stackexchange.com. 600 IN A 151.101.65.69 math.stackexchange.com. 600 IN A 151.101.1.69 ;; Query time: 74 msec ;; SERVER: 127.0.0.53#53(127.0.0.53) ;; WHEN: Mon Sep 13 10:18:12 UTC 2021 ;; MSG SIZE rcvd: 115 记录类型 域名与IP之间的对应关系，称为记录（record）。根据使用场景，记录可以分成不同的类型（type）。\n   记录类型 记录类型简称 描述     地址记录（Address） A 返回域名指向的IP地址   域名服务器记录（Name Server） NS 返回保存下一级域名信息的服务器地址。该记录只能设置为域名，不能设置为IP地址   邮件记录（Mail eXchange） MX 返回接收电子邮件的服务器地址   规范名称记录（Canonical Name） CNAME 返回另一个域名，即当前查询的域名是另一个域名的跳转   逆向查询记录（Pointer Record） PTR 只用于从IP地址查询域名    传输方式 DNS 可以使用 UDP 或者 TCP 进行传输，使用的端口号都为 53。大多数情况下 DNS 使用 UDP 进行传输，这就要求域名解析器和域名服务器都必须自己处理超时和重传从而保证可靠性。在两种情况下会使用 TCP 进行传输：\n 如果返回的响应超过的 512 字节（UDP 最大只支持 512 字节的数据） 区域传送（区域传送是主域名服务器向辅助域名服务器传送变化的那部分数据）  参考资料  【日】户根勤. (2017). 网络是怎样连接的. 人民邮电出版社. Wikipedia : DNS 阮一峰的网络日志: DNS 原理入门  ","date":"2020-06-14T16:02:15+08:00","permalink":"https://huangkai1008.github.io/p/%E5%9F%9F%E5%90%8D%E5%92%8Cdns/","title":"域名和DNS"},{"content":"HTTP 一、概述  超文本传输协议（HyperText Transfer Protocol, HTTP） 是 Web 的核心，HTTP 由客户端程序和服务器程序实现\n HTTP 使用 TCP 作为它的支撑运输协议，因为 HTTP 服务器并不保存关于客户的任何信息，所以 HTTP 是一个无状态协议（stateless protocol）。\n请求和响应报文 客户端发送一个请求报文给服务器，服务器根据请求报文中的信息进行处理，并将处理结果放入响应报文中返回给客户端。\n请求消息（requests） GET / HTTP/1.1 Host: developer.mozilla.org Accept-Language: fr   起始行（start line）：包含一个HTTP方法（method）、请求目标（request target） 和 HTTP 版本 （HTTP version）\n  消息头（headers）： 整个 header（包括其值）表现为单行形式\n  一个空行用来分隔首部和内容主体 Body\n  消息主体（body）\n   响应消息(responses) HTTP/1.1 200 OK Date: Sat, 09 Oct 2010 14:28:02 GMT Server: Apache Last-Modified: Tue, 01 Dec 2009 20:18:22 GMT ETag: \u0026#34;51142bc1-7449-479b075b2891b\u0026#34; Accept-Ranges: bytes Content-Length: 29769 Content-Type: text/html \u0026lt;!DOCTYPE html... (here comes the 29769 bytes of the requested web page)  状态行（status line)：  协议版本，通常为 HTTP/1.1. 状态码 (status code)，表明请求是成功或失败。常见的状态码是 200，404，或 302 状态文本 (status text)：一个简短的，纯粹的信息，通过状态码的文本描述，帮助理解该 HTTP 消息   消息头（Headers）： 整个 header（包括其值）表现为单行形式 一个空行用来分隔首部和内容主体 Body 消息主体（body）   二、HTTP 方法    请求方法 描述 RFC 请求具有请求实体 响应具有响应实体 安全方法 是否幂等     GET 请求一个指定的资源 RFC 7231 可选 是 是 是   HEAD 获取报文首部，不返回报文实体主体，主要用于确认 URL 的有效性以及资源更新的日期时间等 RFC 7231 可选 否 是 是   POST 用于将实体提交到指定的资源 RFC 7231 是 是 否 否   PUT 向指定资源位置上传其最新内容 RFC 7231 是 是 否 是   PATCH 对资源进行部分修改 RFC 5789 是 是 否 否   DELETE 删除指定的资源 RFC 7231 可选 是 否 是   CONNECT 要求在与代理服务器通信时建立隧道 RFC 7231 可选 是 否 否   OPTIONS 查询指定的 URL 能够支持的方法 RFC 7231 可选 是 是 是   TRACE 服务器会将通信路径返回给客户端 RFC 7231 否 是 是 是     三、HTTP 首部（header)    类型 描述 实例     通用头（General headers） 适用于请求和响应信息的头字段 Date,Cache-Control   请求头（Request headers） 用于表示请求信息的附加信息的头字段 Authorization,User-Agent,Accept-Encoding   响应头（Response headers） 用于表示响应信息的附加信息的头字段 Location,Server   实体头（Entity headers） 用于表示实体（消息体）的附加信息的头字段 Allow,Content-Encoding,Expires, Etag    四、HTTP 状态码（status code）    状态码 含义     1xx(informational response) 告知请求的处理进度和情况   2xx(successful) 成功   3xx(redirection) 需要进一步处理   4xx(client error) 客户端错误   5xx(server error) 服务器错误    五、连接管理 非持续连接和持续连接  每个请求及其响应对经一个单独的 TCP 连接发送，此种方式称为使用非持续连接（non-persistent connection），也可以称为短连接；\n多个请求及其响应经过相同的TCP连接发送，此种方式称为使用持续连接（persistent connection），也可以称为长连接；\n 这里的持续连接（长连接）和非持续连接（短连接）指的都是TCP连接。\n从 HTTP/1.1 开始默认使用持续连接，如果要断开连接，需要由客户端或者服务器端提出断开，使用 Connection : close；\n在 HTTP/1.1 之前默认使用非持续连接的，如果需要使用持续连接，则使用 Connection : Keep-Alive。\n非持续连接的问题  必须为每一个请求的对象建立和维护一个全新的连接，会产生大量的开销，给 web 服务器带来严重负担 每一个对象经受两倍 RTT（Round-Trip Time, RTT, 即往返时延）的交付时延（一个 RTT 创建 TCP，一个RTT请求和接受一个对象），效率较低  持续连接的问题  在空闲状态也消耗服务器资源，而且在重负载时，还有可能遭受 DoS 攻击，对于这种情况一般采取的策略是： 1.关闭一些长时间没有发生请求的连接 2.限制每个客户端的最大连接数，避免恶意的客户端影响服务端  六、Cookie 和 Session HTTP Cookies  HTTP Cookie（也叫 Web Cookie 或浏览器 Cookie） 是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器下次向同一服务器再发起请求时被携带并发送到服务器上。通常，它用于告知服务端两个请求是否来自同一浏览器，如保持用户的登录状态。Cookie 使基于无状态的 HTTP 协议记录稳定的状态信息成为了可能\n Cookie 曾一度用于客户端数据的存储，因当时并没有其它合适的存储办法而作为唯一的存储手段，但现在随着现代浏览器开始支持各种各样的存储方式，Cookie 渐渐被淘汰。由于服务器指定 Cookie 后，浏览器的每次请求都会携带 Cookie 数据，会带来额外的性能开销（尤其是在移动环境下）。新的浏览器API已经允许开发者直接将数据存储到本地，如使用 Web storage API （本地存储和会话存储）或 IndexedDB 。\n用途   会话状态管理（如用户登录状态、购物车、游戏分数或其它需要记录的信息）\n  个性化设置（如用户自定义设置、主题等）\n  浏览器行为跟踪（如跟踪分析用户行为等）\n  创建过程 当服务器收到 HTTP 请求时，服务器可以在响应头里面添加一个 Set-Cookie 选项。浏览器收到响应后通常会保存下 Cookie，之后对该服务器每一次请求中都通过 Cookie 请求头部将 Cookie 信息发送给服务器。另外，Cookie 的过期时间、域、路径、有效期、适用站点都可以根据需要来指定。\nSet-Cookie响应头部和Cookie请求头部 服务器使用 Set-Cookie 响应头部向用户代理（一般是浏览器）发送 Cookie信息。一个简单的 Cookie 可能像这样：\nSet-Cookie: \u0026lt;cookie名\u0026gt;=\u0026lt;cookie值\u0026gt; 服务器通过该头部告知客户端保存 Cookie 信息，客户端得到响应报文后把 Cookie 内容保存到浏览器中：\nHTTP/1.0 200 OK Content-type: text/html Set-Cookie: yummy_cookie=choco Set-Cookie: tasty_cookie=strawberry [page content] 现在，对该服务器发起的每一次新请求，浏览器都会将之前保存的Cookie信息通过 Cookie 请求首部字段再发送给服务器：\nGET /sample_page.html HTTP/1.1 Host: www.example.org Cookie: yummy_cookie=choco; tasty_cookie=strawberry 生命周期  会话期 Cookie：浏览器关闭之后它会被自动删除，也就是说它仅在会话期内有效 持久性 Cookie：指定过期时间（Expires）或有效期（max-age）之后就成为了持久性的 Cookie  Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; 作用域 Domain 标识 Domain标识指定了哪些主机可以接受 Cookie。如果不指定，默认为当前文档的主机（不包含子域名）。如果指定了 Domain，则一般包含子域名。例如，如果设置 Domain=mozilla.org，则 Cookie 也包含在子域名中（如 developer.mozilla.org）。\nPath 标识 Path 标识指定了主机下的哪些路径可以接受 Cookie（该 URL 路径必须存在于请求 URL 中）。以字符 %x2F (\u0026quot;/\u0026quot;) 作为路径分隔符，子路径也会被匹配。例如，设置 Path=/docs，则以下地址都会匹配：\n /docs /docs/Web/ /docs/Web/HTTP  限制访问 Secure 属性 标记为 Secure 的 Cookie 只应通过被 HTTPS 协议加密过的请求发送给服务端，因此可以预防中间人的攻击。但即便设置了 Secure 标记，敏感信息也不应该通过 Cookie 传输，因为 Cookie 有其固有的不安全性，Secure 标记也无法提供确实的安全保障, 例如，可以访问客户端硬盘的人可以读取它。\nHttpOnly 属性 标记为 HttpOnly 的 Cookie 不能被 JavaScript 脚本调用。跨站脚本攻击 (XSS) 常常使用 JavaScript 的 document.cookie API 窃取用户的 Cookie 信息，因此使用 HttpOnly 标记可以在一定程度上避免 XSS 攻击。\n示例：\nSet-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; Secure; HttpOnly 以下为一个Golang实现的简单的使用 Cookie 的代码示例：\n Session  Session 代表着服务器和客户端一次会话的过程。Session 对象存储特定用户会话所需的属性及配置信息。这样，当用户在应用程序的 Web 页之间跳转时，存储在 Session 对象中的变量将不会丢失，而是在整个用户会话中一直存在下去。当客户端关闭会话，或者 Session 超时失效时会话结束\n Cookie 和 Session 的不同  作用范围不同，Cookie 保存在客户端，Session 保存在服务端 存取方式的不同，Cookie 只能保存 ASCII，Session 可以存任意数据类型，一般情况下我们可以在 Session 中保持一些常用变量信息 有效期不同，Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，Session 一般失效时间较短，客户端关闭或者 Session 超时都会失效 隐私策略不同，Cookie 存储在客户端，比较容易遭到不法获取；Session 存储在服务端，安全性相对要好一些 存储大小不同， 单个 Cookie 保存的数据不能超过 4KB，Session 可存储上限远高于 Cookie  七、Web 缓存  Web缓存（Web cache）（或 HTTP 缓存（HTTP cache））是用于临时存储（缓存）Web文档（如HTML页面和图像），以减少服务器延迟的一种信息技术。Web缓存系统会保存下通过这套系统的文档的副本；如果满足某些条件，则可以由缓存满足后续请求。 Web缓存系统既可以指设备，也可以指计算机程序\n 缓存的种类 缓存的种类有很多,其大致可归为两类：私有与共享缓存。共享缓存存储的响应能够被多个用户使用。私有缓存只能用于单独用户。\n（私有）浏览器缓存 私有缓存（local cache/private cache） 只能用于单独用户。浏览器缓存拥有用户通过 HTTP 下载的所有文件，可以避免再次向服务器发起多余的请求，也可以提供缓存内容的离线浏览。\n（共享）代理缓存 共享缓存（shared cache/proxy cache） 可以被多个用户使用。例如，ISP 或者公司可能会架设一个 web 代理来作为本地网络基础的一部分提供给用户，这样热门的资源就会被重复使用，减少网络拥堵与延迟。\n缓存控制 Cache-Control HTTP/1.1定义的 Cache-Control 头用来区分对缓存机制的支持情况， 请求头和响应头都支持这个属性。通过它提供的不同的值来定义缓存策略。\n禁止进行缓存 缓存中不得存储任何关于客户端请求和服务端响应的内容。每次由客户端发起的请求都会下载完整的响应内容。\nCache-Control: no-store 强制确认缓存 缓存服务器需要先向源服务器验证缓存资源的有效性，只有当缓存资源有效时才能使用该缓存对客户端的请求进行响应。\nCache-Control: no-cache 私有缓存和公共缓存 private 指令规定了将资源作为私有缓存，只能被单独用户使用，一般存储在用户浏览器中。\nCache-Control: private public 指令规定了将资源作为公共缓存，可以被任何中间人（比如中间代理、CDN等）缓存，可以被多个用户使用，一般存储在代理服务器中。\nCache-Control: public 过期 max-age 指令出现在请求报文，并且缓存资源的缓存时间小于该指令指定的时间，那么就能接受该缓存，\nmax-age 指令出现在响应报文，表示缓存资源在缓存服务器中保存的时间：\nCache-Control: max-age=31536000 Expires 首部字段也可以用于告知缓存服务器该资源什么时候会过期：\nExpires: Wed, 04 Jul 2012 08:26:05 GMT 在HTTP/1.1中，会优先处理 max-age 指令，在HTTP/1.0中，会忽略掉 max-age 指令。\n验证方式 当使用了 must-revalidate 指令，那就意味着缓存在考虑使用一个陈旧的资源时，必须先验证它的状态，已过期的缓存将不被使用。\nCache-Control: must-revalidate 新鲜度(freshness)  服务端和客户端为资源约定一个过期时间，在该过期时间之前，该资源（缓存副本）就是新鲜的，当过了过期时间后，该资源（缓存副本）则变为陈旧的。驱逐算法用于将陈旧的资源（缓存副本）替换为新鲜的，注意，一个陈旧的资源（缓存副本）是不会直接被清除或忽略的\n 对于含有特定头信息的请求，会去计算缓存寿命。比如Cache-control: max-age=N的头，相应的缓存的寿命就是N。\n缓存失效时间计算公式如下： $$ expirationTime = responseTime + freshnessLifetime - currentAge $$ 其中，responseTime 表示浏览器接收到此响应的时间点。\n缓存验证(validation) ETag  ETag 响应头是 URL 的Entity Tag，作为一个URL资源的标识符，作为缓存的一种强校验器\n ETag: \u0026#34;82e22293907ce725faf67773957acd12\u0026#34; 当服务端返回资源时，可以根据返回内容计算一个 hash 值或者就是一个数字版本号作为 ETag 的值放到响应首部中，客户端可以在后续的请求的头中可以将缓存资源的 ETag 值放到 If-None-Match 头首部，服务器收到该请求后，判断缓存资源的 ETag 值和资源的最新 ETag 值是否一致，如果一致则表示缓存资源有效，返回 304 Not Modified。\nIf-None-Match: \u0026#34;82e22293907ce725faf67773957acd12\u0026#34; Last-Modified Last-Modified 响应头可以作为缓存验证的一种弱校验器，如果响应头里含有这个信息，客户端可以在后续的请求中带上 If-Modified-Since 来验证缓存。服务器只在所请求的资源在给定的日期时间之后对内容进行过修改的情况下才会将资源返回，状态码为 200 OK。如果请求的资源从那时起未经修改，那么返回一个不带有实体主体的 304 Not Modified 响应报文。\nLast-Modified: Wed, 21 Oct 2015 07:28:00 GMT If-Modified-Since: Wed, 21 Oct 2015 07:28:00 GMT Etag 与 Last-Modified的对比 Etag 是强校验器，Last-Modified 是弱校验器，都同时出现时，Etag的优先级更高。Last-Modified的精度只能到秒，如果一个资源频繁修改，用Last-Modified并不能区分，而Etag 由于每次资源更新时都会生成新的值，会使缓存验证更加准确，缺点是频繁生成的策略可能会额外消耗服务器资源。\n强制缓存与协商缓存  强制缓存：浏览器不会向服务器发送任何请求，直接从本地缓存中读取文件并返回状态码200 OK；\n协商缓存：浏览器向服务器发送请求，服务器会根据这个请求的请求首部来判断是否命中协商缓存，如果命中，则返回304 Not Modified并带上新的响应首部通知浏览器从缓存中读取资源\n 强制缓存的首部字段  Expires Cache Control  协商缓存的首部字段   Etag \u0026amp; If-None-Match\n  Last-Modifed \u0026amp; If-Modified-Since\n  强制缓存和协商缓存都存在的情况下，先判断强制缓存是否生效，如果生效，不用发起请求，直接用缓存。如果强制缓存不生效再发起请求判断协商缓存。\n八、HTTP/2 HTTP/1.1 的持久连接和管道机制允许复用TCP连接，在一个TCP连接中，也可以同时发送多个请求，但是所有的数据通信都是按次序完成的，服务器只有处理完一个回应，才会处理下一个回应。比如客户端需要A、B两个资源，管道机制允许浏览器同时发出 A 请求和 B 请求，但服务器还是按照顺序，先回应 A 请求，完成后再回应 B 请求，这样如果前面的回应特别慢，后面就会有很多请求排队等着，这称为队头阻塞（Head-of-line blocking）。\n参考资料  【日】户根勤. (2017). 网络是怎样连接的. 人民邮电出版社. Kurose, J. F., \u0026amp; Ross, K. W. (2018). 计算机网络-自顶而下方法 (7th ed.). 机械工业出版社. Wikipedia : HTTP MDN : HTTP MDN : Cookies MDN : HTTP Caching  ","date":"2020-06-12T12:22:57+08:00","permalink":"https://huangkai1008.github.io/p/http/","title":"HTTP"},{"content":"MySQL的日志系统 一、日志类型  MySQL主要有两种日志类型，一种是物理日志（记录在某个数据页上做了什么修改)，一种是逻辑日志(存储了逻辑SQL修改语句)。\nredo log属于物理日志，binlog和undo log属于逻辑日志，其中物理日志的恢复速度远快于逻辑日志。\n 二、重做日志(redo log) 基本概念  重做日志（redo log）是 InnoDB 引擎层的日志，用来记录事务操作引起数据的变化，记录的是数据页的物理修改，提供前滚操作。MySQL 通过 redo log 保证事务的持久性。\n 重做日志由两部分组成，一是内存中的重做日志缓冲区 (redo log buffer)，它是易失的，另一个就是在磁盘上的重做日志文件 (redo log file)，它是持久的。\nInnoDB 引擎对数据更新，是先将更新记录写入到重做日志，在系统空闲时或者按照设定的更新策略再将日志中的内容更新到磁盘中，这就是预写式技术 (Write Ahead logging, WAL)，这种技术可以大大减少IO操作的频率，提升数据刷新的效率。\n逻辑结构 redo log 的大小是固定的，为了能够持续不断的对更新记录进行写入，在redo log日志中设置了两个标志位置，checkpoint和write pos。checkpoint表示记录擦除的位置，write pos表示记录写入的位置。当write pos标志到了日志结尾时，会从结尾跳至日志头部循环写入，所以redo log的逻辑结构并不是线性的，可以看做一个圆周运动，逻辑结构见下图：\n当write_pos追上checkpoint时，表示redo log日志已经写满。这时不能继续执行新的数据库更新语句，需要停下来先删除一些记录，执行checkpoint规则腾出可写空间。\n checkpoint规则：checkpoint触发后，将buffer中脏数据页和脏日志页都刷到磁盘。所谓的脏数据页就是指内存中未刷到磁盘的数据\n 刷盘  redo log buffer 数据页写入磁盘中的redo log file的过程叫做刷盘。\n 在计算机操作系统中，用户空间(user space)下的缓冲区数据一般情况下是无法直接写入磁盘的，中间必须经过操作系统内核空间(kernel space)的缓冲区(OS Buffer)。因此，redo log buffer写入redo log file实际上是先写入OS Buffer，然后再通过系统调用fsync()将其刷到redo log file中，流程如下图：\n当数据修改时，除了修改buffer pool中的数据，还会在redo log中记录这次操作。如果MySQL宕机，重启时可以读取redo log中的数据，对数据库进行恢复，从而保证了事务的持久性，使得数据库获得crash-safe能力。\n刷盘策略 在提交事务的时候，InnoDB会根据配置的策略来将 redo log 刷盘，这个可以通过innodb_flush_log_at_trx_commit 参数来配置。\nSHOWVARIABLESLIKE\u0026#39;innodb_flush_log_at_trx_commit\u0026#39;;+--------------------------------+-------+|Variable_name|Value|+--------------------------------+-------+|innodb_flush_log_at_trx_commit|1|+--------------------------------+-------+1rowinset(0.00sec)各参数的含义如下表：\n   参数值 含义     0（延迟写） 事务提交时不会将redo log buffer中日志写入到OS Buffer，而是定时写入OS buffer并调用fsync()写入到redo log file中。当系统崩溃，会丢失数据。   1 （实时写，实时刷） 事务每次提交都会将redo log buffer中的日志写入OS buffer并调用fsync()刷到redo log file中。这种方式即使系统崩溃也不会丢失任何数据，但是因为每次提交都写入磁盘，IO的性能较差。   2 （实时写，延时刷） 每次提交都仅写入到OS buffer，然后是每秒调用fsync()将os buffer中的日志写入到redo log file。    为了保证事务的持久性，一般使用默认值，将 innodb_flush_log_at_trx_commit 设置为1即可。\n三、二进制日志 (binlog) 基本概念  binlog（二进制日志、归档日志），用于记录数据库执行的写入性操作信息，以二进制的形式保存在磁盘中。\n 使用场景  主从复制：从库利用主库上的 binlog 进行重播，实现主从同步 数据恢复：用于数据库的基于时间点、位点等的还原操作（mysqlbinlog）  日志格式 binlog日志有三种格式，分别为STATMENT、ROW和MIXED。\n 在 MySQL 5.7.7之前，默认的格式是STATEMENT，MySQL 5.7.7之后，默认值是ROW。日志格式通过binlog-format指定。\n STATEMENT STATMENT 基于SQL语句的复制(statement-based replication, SBR)，会记录每一条修改数据的SQL语句和执行语句的上下文信息\n优点 STATMENT 模式不需要记录每一行的变化，减少了binlog的日志量，节省了I/O和存储资源,，从而提高了性能\n缺点 STATMENT 模式在某些情况下会导致主从数据不一致，比如执行sysdate()、sleep()等\nROW ROW 基于行的复制(row-based replication, RBR)，会记录每一行数据被修改的形式\n优点 ROW 模式下的日志内容会非常清楚的记录下每一行数据的修改细节，非常容易理解，而且不会出现某些特定情况下的存储过程和 function，以及 trigger 的调用和触发无法被正确复制问题\n缺点 ROW 模式下会产生大量的日志\nMIXED MIXED 基于STATMENT和ROW两种模式的混合复制(mixed-based replication, MBR)，一般的复制使用STATEMENT模式保存，对于STATEMENT模式无法复制的操作使用ROW模式保存\n刷盘策略 MySQL 只有在事务提交的时候才会记录 binlog 日志，此时日志还在内存中，MySQL 通过sync_binlog参数控制 biglog 的刷盘时机，取值范围是0-N：\n  0： 不做强制要求，由系统自行判断何时写入磁盘\n  1：每次事务提交时 binlog 都会写入磁盘\n  N：每N个事务 binlog 会写入磁盘\n  sync_binlog最安全的是设置是1，这也是 MySQL 5.7.7之后版本的默认值，但是也可以设置一个大一些的值可以提升数据库性能，因此实际情况下也可以将值适当调大，牺牲一定的一致性来获取更好的性能。\n与重做日志的区别     redo log binlog     实现方式 InnoDB引擎特有的 MySQL的Server层实现的，所有引擎都可以使用   日志类型 物理日志 逻辑日志   写入方式 循环写，空间固定会用完 追加写入，binlog文件写到一定大小后会切换到下一个   适用场景 崩溃恢复(crash-safe) 主从复制和数据恢复    两阶段提交（Two-phase Commit，2PC） MySQL 事务提交的时候，需要同时完成 redo log 和 binlog 的提交，为了让两份日志之间的逻辑一致，需要用到两阶段提交，这个场景下的两阶段提交发生在 MySQL 内部，和分布式系统的两阶段提交是两个概念。\n四、回滚日志 (undo log)  当事务对数据库进行修改，InnoDB引擎不仅会记录redo log，还会生成对应的undo log日志；如果事务执行失败或调用了rollback，导致事务需要回滚，就可以利用undo log中的信息将数据回滚到修改之前的状态。MySQL 通过 undo log 保证事务的原子性。undo log有两个作用，一是提供回滚，二是实现 MVCC\n 回滚日志并不能将数据库物理地恢复到执行语句或者事务之前的样子；它是逻辑日志，当回滚日志被使用时，它只会按照日志逻辑地将数据库中的修改撤销掉，可以理解为，我们在事务中使用的每一条 INSERT 都对应了一条 DELETE，每一条 UPDATE 也都对应一条相反的 UPDATE 语句。\n事务日志 在数据库系统中，事务的原子性和持久性是由事务日志（transaction log）保证的，而redo log和undo log都属于InnoDB引擎层下的事务日志（transaction log）。这两种事务日志可以保证：\n 发生错误或者需要回滚的事务能够成功回滚（原子性） 在事务提交后，数据没来得及写入磁盘就宕机时，在下次重新启动后能够成功恢复数据（持久性）  在数据库中，这两种日志经常都是一起工作的，可以将它们整体看做一条事务日志，其中包含了事务的 ID、修改的行元素以及修改前后的值。\n一条事务日志同时包含了修改前后的值，能够非常简单的进行回滚和重做两种操作。\n","date":"2020-06-05T22:07:21+08:00","permalink":"https://huangkai1008.github.io/p/mysql%E7%9A%84%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/","title":"MySQL的日志系统"},{"content":"概念  事务就是一组原子性的SQL查询，或者说一个独立的工作单元。如果数据库引擎能够成功地对数据库应用该组查询的全部语句，那么就执行该组查询。如果其中有任何一条语句因为崩溃或其他原因无法执行，那么所有的语句都不会执行。也就是说，事务内的语句，要么全部执行成功，要么全部执行失败。在 MySQL 中，事务支持是在引擎层实现的。\n ACID特性   原子性（Atomicity）：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行\n  一致性（Consistency）：事务应确保数据库的状态从一个一致状态转变为另一个一致状态。一致状态的含义是数据库中的数据应满足完整性约束。\n  隔离性（Isolation）：通常来说，一个事务所做的修改在最终提交以前，对其他事务是不可见的。\n  持久性（Durability）：已被提交的事务对数据库的修改应该永久保存在数据库中。\n   一个实现了ACID的数据库，相比没有实现ACID的数据库，通常会需要更强的CPU处理能力、更大的内存和更多的磁盘空间。\n 隔离级别（Isolation level）   READ UNCOMMITTED（读未提交）\n​\t事务中的修改，即使没有提交，对其他事务也都是可见的。事务可以读取未提交的数据，这也被称为脏读（Dirty Read）。\n  READ COMMITTED（读提交）\n​\tOracle和SQL Server的默认隔离级别。一个事务可以读取另一个已提交的事务。换句话说，一个事务从开始直到提交之前，所做的任何修改对其他事务都是不可见的。这个级别有时候也叫做不可重复读（nonrepeatable read），因为两次执行同样的查询，可能会得到不一样的结果。MySQL的InnoDB引擎在提交读级别通过MVCC解决了不可重复读的问题\n  REPEATABLE READ（可重复读）\n​\tMySQL的默认隔离级别。一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。MySQL的InnoDB引擎在提交读级别通过MVCC解决了不可重复读的问题，在可重复读级别通过间隙锁解决了幻读问题。\n  SERIALIZABLE（可串行化）\n​\tSERIALIZABLE是最高的隔离级别。它通过强制事务串行执行，避免了前面说的幻读的问题。简单来说，SERIALIZABLE会在读取的每一行数据上都加锁，所以可能导致大量的超时和锁争用的问题。实际应用中也很少用到这个隔离级别，只有在非常需要确保数据的一致性而且可以接受没有并发的情况下，才考虑采用该级别。\n     隔离级别 脏读可能性 不可重复读可能性 幻读可能性 加锁读     READ UNCOMMITTED √ √ √ ×   READ COMMITTED × √ √ ×   REPEATABLE READ × × √ ×   SERIALIZABLE × × × √     查看MySQL的隔离级别\n SHOWVARIABLESLIKE\u0026#39;transaction_isolation\u0026#39;;+-----------------------+-----------------+|Variable_name|Value|+-----------------------+-----------------+|transaction_isolation|REPEATABLE-READ|+-----------------------+-----------------+1rowinset,1warning(0.00sec) 设置当前会话的隔离级别\n SETSESSIONTRANSACTIONISOLATIONLEVELREADCOMMITTED;# 设置当前会话为RC级别，下个事务生效 事务类型 隐式事务 DML操作的语句都会隐式的开启事务，并且在语句执行后没有错误的话隐式提交。可以通过将MySQL的autocommit这个变量（默认为1）设置为0将事务的隐式提交关闭，但需要注意，DML语句的隐式事务仍会启动，只是区别在于需要手动COMMIT显式提交这个事务，也就是将隐式事务转化为长事务了。\nSHOWVARIABLESLIKE\u0026#39;autocommit\u0026#39;;# 查看隐式事务提交方式 +---------------+-------+|Variable_name|Value|+---------------+-------+|autocommit|ON|+---------------+-------+1rowinset,1warning(0.00sec)显式事务 # 1.显式开启一个事务 STARTTRANSACTION;BEGIN;# 2.提交事务 COMMIT;# 3.回滚事务 ROLLBACK;# 4.在事务中创建保存点，可以在同一事务中创建多个，以便通过ROLLBACK更灵活的回滚 SAVEPOINT;显式开启一个事务时，如果还有未提交的事务会自动提交，并且autocommit会被禁用直到该事务结束。对于显式事务，存在completion_type这样一个变量控制显式事务的行为。有下列三种情况：\n 值为0时即为默认，执行COMMIT后提交该显式事务并结束该事务。 值为1时，执行COMMIT后除了有值为0时的默认行为外，随后会自动开始一个相同隔离级别的事务。术语为COMMIT AND CHAIN 值为2时，执行COMMIT后除了有值为0时的默认行为外，随后会断开与服务器的连接。术语为COMMIT AND RELEASE  ","date":"2020-06-02T22:25:20+08:00","permalink":"https://huangkai1008.github.io/p/%E4%BA%8B%E5%8A%A1/","title":"事务"},{"content":"MySQL的基础架构 逻辑架构 MySQL可以大体分为Server层和存储引擎层两部分, 见图1\n Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎  连接器(Connector) 连接器负责和客户端建立连接、获取权限、维持和管理连接。\n# Mysql连接命令 mysql-h$ip-P$port-u$user-pMySQL客户端和服务端完成TCP握手后，连接器需要认证身份\n  如果用户名或密码不对，你就会收到一个 Access denied for user 的错误，然后客户端程序结束执行。\n  如果用户名密码认证通过，连接器会到权限表里面查出拥有的权限，之后这个连接里面的权限判断逻辑，都将依赖于此时读到的权限，这就意味着，一个用户成功建立连接后，即使这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。\n  长连接  长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短 连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。\n 查询缓存(Query Cache) 在连接建立完成后，MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果你的查询能够直接在这个缓存中找到 key，那么这个 value 就会被直接返回给客户端。\n 注：MySQL 8.0 版本的查询缓存功能被移除了\n 分析器(Parser) 分析器的主要功能是对SQL语句做解析\n 分析器会先做词法分析，再做语法分析，语法分析器会根据语法规则，判断 SQL 语句是否满足 MySQL 语法  优化器(Query Optimizer) 优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。优化器并不关心表使用的是什么存储引擎，但存储引擎对于优化查询是有影响的。优化器会请求存储引擎提供容量或某个具体操作的开销信息，以及表数据的统计信息等。例如，某些存储引擎的某种索引，可能对一些特定的查询有优化。\n执行器(Query execution engine) 开始执行的时候，要先判断一下对于表有没有执行操作的权限，如果没有，就会返回没有权限的错误。如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。\n对于一个特定的例子\nselect*fromuserwhereID=10;假定ID字段没有索引，那么执行器的执行流程是这样的：\n 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中； 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。  ","date":"2020-06-01T21:56:20+08:00","permalink":"https://huangkai1008.github.io/p/mysql%E7%9A%84%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84/","title":"MySQL的基础架构"},{"content":"IDE  Jetbrains系列  Editor  Vscode Sublime text Typora(markdown编辑) Notion(笔记软件)  画图  Microsoft visio drawing.io  数据库 Mysql系列  Navicat Tableplus Jetbrains Datagrip  Redis系列  RDM(redis desktop manager)  CVS  Jetbrains系IDE自带 SourceTree  接口测试  Postman  Vm  VirtualBox Vagrant Windows SubLinux(WSL) Vmware  ssh工具  MobaXterm xshell  系统工具  Everything Utools  接口文档工具  apidoc swagger yapi  终端  cmder Windows Termial  ","date":"2019-12-01T22:21:57+08:00","permalink":"https://huangkai1008.github.io/p/awesome-software/","title":"Awesome Software"},{"content":"Loguru是一个好用的第三方python日志库\n安装 pip install loguru 初步使用 添加日志到标准输出流 import sys from loguru import logger logger.add(sys.stderr, format=\u0026#39;{time}{level}{message}\u0026#39;, filter=\u0026#39;my module\u0026#39;, level=\u0026#39;INFO\u0026#39;) 添加日志到文件 from loguru import logger logger.add(\u0026#39;file_1.log\u0026#39;, rotation=\u0026#39;500 MB\u0026#39;) # Automatically rotate too big file logger.add(\u0026#34;file_2.log\u0026#34;, rotation=\u0026#39;12:00\u0026#39;) # New file is created each day at noon logger.add(\u0026#34;file_3.log\u0026#34;, rotation=\u0026#34;1 week\u0026#34;) # Once the file is too old, it\u0026#39;s rotated logger.add(\u0026#34;file_X.log\u0026#34;, retention=\u0026#34;10 days\u0026#34;) # Cleanup after some time logger.add(\u0026#34;file_Y.log\u0026#34;, compression=\u0026#34;zip\u0026#34;) # Save some loved space 捕获异常 from loguru import logger @logger.catch def my_function(x, y, z): # An error? It\u0026#39;s caught anyway! return 1 / (x + y + z) 为日志添加颜色 import sys from loguru import logger logger.add(sys.stdout, colorize=True, format=\u0026#34;\u0026lt;green\u0026gt;{time}\u0026lt;/green\u0026gt; \u0026lt;level\u0026gt;{message}\u0026lt;/level\u0026gt;\u0026#34;) 异步、线程安全、多进程安全 from loguru import logger logger.add(\u0026#34;file.log\u0026#34;, enqueue=True) 完全描述异常  记录代码中发生的异常对于跟踪错误很重要，但是如果您不知道为什么失败，则记录日志就毫无用处。 Loguru通过允许显示整个堆栈跟踪（包括变量值）来帮助您发现问题\n from loguru import logger logger.add(\u0026#34;output.log\u0026#34;, backtrace=True, diagnose=True) # Set \u0026#39;False\u0026#39; to not leak sensitive data in prod 配置到flask import logging import sys from pathlib import Path from flask import Flask from loguru import logger app = Flask(__name__) class InterceptHandler(logging.Handler): def emit(self, record): logger_opt = logger.opt(depth=6, exception=record.exc_info) logger_opt.log(record.levelname, record.getMessage()) def configure_logging(flask_app: Flask): \u0026#34;\u0026#34;\u0026#34;配置日志\u0026#34;\u0026#34;\u0026#34; path = Path(flask_app.config[\u0026#39;LOG_PATH\u0026#39;]) if not path.exists(): path.mkdir(parents=True) log_name = Path(path, \u0026#39;sips.log\u0026#39;) logging.basicConfig(handlers=[InterceptHandler(level=\u0026#39;INFO\u0026#39;)], level=\u0026#39;INFO\u0026#39;) logger.configure(handlers=[{\u0026#34;sink\u0026#34;: sys.stderr, \u0026#34;level\u0026#34;: \u0026#39;INFO\u0026#39;}]) # 配置日志到标准输出流 logger.add( log_name, rotation=\u0026#34;500 MB\u0026#34;, encoding=\u0026#39;utf-8\u0026#39;, colorize=False, level=\u0026#39;INFO\u0026#39; ) # 配置日志到输出到文件 ","date":"2019-11-22T15:19:35+08:00","permalink":"https://huangkai1008.github.io/p/%E6%97%A5%E5%BF%97%E5%BA%93loguru%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/","title":"日志库Loguru使用教程"},{"content":"Evenlet是一个Python的基于携程的网络库，它改变了你代码运行的方式，但是没有改变你怎么写代码\n安装 pip install eventlet 简单使用 从eventlet.green导入相关库 import eventlet from eventlet.green import urllib2 urls = [ \u0026#34;https://www.google.com/intl/en_ALL/images/logo.gif\u0026#34;, \u0026#34;http://python.org/images/python-logo.gif\u0026#34;, \u0026#34;http://us.i1.yimg.com/us.yimg.com/i/ww/beta/y3.gif\u0026#34;, ] def fetch(url): print(\u0026#34;opening\u0026#34;, url) body = urllib2.urlopen(url).read() print(\u0026#34;done with\u0026#34;, url) return url, body pool = eventlet.GreenPool(200) for url, body in pool.imap(fetch, urls): print(\u0026#34;got body from\u0026#34;, url, \u0026#34;of length\u0026#34;, len(body)) 使用spawn使用协程 import time import eventlet def green_thread_1(num): eventlet.greenthread.sleep(1) print(f\u0026#39;green_thread_1 get result {num}\u0026#39;) return x def green_thread_2(num): eventlet.greenthread.sleep(2) print(f\u0026#39;green_thread_2 get result {num}\u0026#39;) return y time1 = time.perf_counter() x = eventlet.spawn(green_thread_1, 1) y = eventlet.spawn(green_thread_2, 2) x.wait() y.wait() time2 = time.perf_counter() print(time2 - time1) \u0026gt;\u0026gt;\u0026gt; green_thread_1 get result 1 green_thread_2 get result 2 2.0049271  spawn函数产生的协程可以通过wait函数来执行并获取返回结果， 如上例子中， 使用绿色线程的休眠模拟io操作的耗时, 程序就会切换到下一个协程，切换协程由调度器决定\n 使用monkey-patch from eventlet import monkey_patch from eventlet import GreenPool green_pool = GreenPool() monkey_patch() def producer(): pass def consumer(): pass green_pool.spawn(producer) green_pool.spawn(consumer) green_pool.waitall() 和gunicorn一起使用 以flask应用为例\ngunicorn --worker-class eventlet -b 0.0.0.0:5000 -w 1 run:app ","date":"2019-11-22T11:20:20+08:00","permalink":"https://huangkai1008.github.io/p/eventlet%E4%BD%BF%E7%94%A8/","title":"Eventlet使用"},{"content":"GitFlow 基本介绍 Gitflow 提倡使用 feature branches 模式来开发各个相互独立的功能，同时分成不同的分支以便进行集成和发布\n分支介绍   长期分支\n   主分支(master)\n  开发分支(develop)\n   在gitflow下, develop 分支是一个类似全能的分支，用来存放、测试所有的代码，同时也是主要是用来合并代码、集成功能的分支\n作为一个开发人员，在这是不允许直接提交代码到 develop 分支上的，更更更不允许直接提交到 master 分支。master 分支代表的是一个「stable」的分支，包含的是已投产或即将投产的代码。如果一段代码在 master 分支上，即代表它已经投产或即将投产发布\n  短期分支\n   功能分支(feature)\n  热补丁分支(hotfix)\n  预发分支(release)\n     feature\n功能性分支从 develop 分支上产生， 根据新需求来新建 feature 分支， 开发完成后，要再并入 develop 分支， 合并完分支后一般会删除这个feature分支\n在 feature 分支的命名规则上，可以约定以 「feat-」开头，后面跟上问题单编号。如「feat-APS-151-add-name-field」。以「feat-」开头，可以让 CI 服务器识别出这是一个 feature 分支，「APS-151」是Jira 问题单的编号，可以链接到问题单，剩下的部分则是对该功能的简短的说明\n  release\nrelease分支基于develop创建\n打完release分支之后，我们可以在这个release分支上测试，修改bug等。同时，其它开发人员可以基于develop分支新建feature (记住：一旦打了release分支之后不要从develop分支上合并新的改动到release分支)发布release分支时，合并release到master和develop， 同时在master分支上打个tag记住release版本号，然后可以删除release分支了。它的命名，可以采用release-*的形式\n在测试中，难免发现 bug，我们可以直接在 release 分支上修改，修改完后再 merge 到 develop 分支上（develop 分支包含的是已发布或者即将发布的代码）\n  hotfix\n这个分支是负责在生产环境上发现的问题，如 bug 或者性能问题等。 hotfixes 分支和 release 分支类似，都以 release 版本号命名，唯一的区别就是 hotfixes 是新建于 master 分支，release 分支则是从 develop 分支而来，修补结束以后，再合并进Master和Develop分支。它的命名，可以采用hotfix-*的形式\n    ​\n","date":"2019-10-12T14:36:49+08:00","permalink":"https://huangkai1008.github.io/p/git%E5%B7%A5%E4%BD%9C%E6%B5%81/","title":"Git工作流"},{"content":"Black是一个毫不妥协的python代码格式化工具, 特点是可配置项较少 Black依赖于python3.6+, 官方地址在https://github.com/psf/black\nInstall pip install black Configure   pyproject.toml\n[tool.black] skip-string-normalization = true # 禁用双引号风格   pycharm\n  Create external tools\n windows: File -\u0026gt; Settings -\u0026gt; Tools -\u0026gt; External Tools\n   Configure file watcher\n    ","date":"2019-09-27T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/black%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/","title":"Black安装和使用"},{"content":"安装   custom installer\ncurl -sSL https://raw.githubusercontent.com/sdispater/poetry/master/get-poetry.py | python source $HOME/.poetry/env   pip\npip install poetry # 不推荐, 可能会有冲突   验证安装\npoetry --version 使用   项目初始化\n  从pipenv/pip等工具迁移\npoetry init # 进入交互式命令行填写项目信息, 会生成pyproject.toml    添加依赖\n  添加包\npoetry add poetry add fastapi=0.38.1 -E all # pipenv install fastapi[all] poetry add celery --extras \u0026#34;librabbitmq redis auth msgpack\u0026#34; # pip install \u0026#34;celery[librabbitmq,redis,auth,msgpack]\u0026#34;   依赖安装\npoetry install # 会从pyproject.toml文件里读取, 如果有poetry.lock文件则会从lock文件中读取锁定依赖并安装   虚拟环境地址\n windows10: $User\\AppData\\Local\\pypoetry\\Cache\\virtualenvs      配置   添加源\n修改pyproject.toml\n[[tool.poetry.source]] name = \u0026#34;tsinghua\u0026#34; url = \u0026#34;https://pypi.tuna.tsinghua.edu.cn/simple/\u0026#34; verify_ssl = true   完整的实例 [tool.poetry] name = \u0026#34;market-admin\u0026#34; version = \u0026#34;0.1.0\u0026#34; description = \u0026#34;market-admin is a Market background management system with fastapi\u0026#34; authors = [\u0026#34;huangkai\u0026#34;] license = \u0026#34;MIT\u0026#34; [tool.poetry.dependencies] python = \u0026#34;^3.7\u0026#34; fastapi = {version = \u0026#34;0.38.1\u0026#34;, extras = [\u0026#34;all\u0026#34;]} python-dotenv = \u0026#34;0.10.2\u0026#34; tortoise-orm = \u0026#34;0.13.5\u0026#34; aiomysql = \u0026#34;0.0.20\u0026#34; loguru = \u0026#34;^0.3.2\u0026#34; [tool.poetry.dev-dependencies] pytest = \u0026#34;6.2.1\u0026#34; coverage = \u0026#34;5.3.1\u0026#34; [tool.black]\t# Black工具配置 target-version = [\u0026#39;py37\u0026#39;] skip-string-normalization = true [[tool.poetry.source]]\t# 源配置 name = \u0026#34;tsinghua\u0026#34; url = \u0026#34;https://pypi.tuna.tsinghua.edu.cn/simple/\u0026#34; default = true [build-system] requires = [\u0026#34;poetry\u0026gt;=0.12\u0026#34;] build-backend = \u0026#34;poetry.masonry.api\u0026#34; ","date":"2019-09-14T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/poetry%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/","title":"Poetry安装和使用"},{"content":"Nginx (engine x) 是一个高性能的HTTP和反向代理web服务器，同时也提供了IMAP/POP3/SMTP服务。Nginx是由伊戈尔·赛索耶夫为俄罗斯访问量第二的Rambler.ru站点（俄文：Рамблер）开发的，第一个公开版本0.1.0发布于2004年10月4日。 其将源代码以类BSD许可证的形式发布，因它的稳定性、丰富的功能集、示例配置文件和低系统资源的消耗而闻名。2011年6月1日，nginx 1.0.4发布。 Nginx是一款轻量级的Web 服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器，在BSD-like 协议下发行。其特点是占有内存少，并发能力强\nInstall   platform: Centos7\n  version: 7.2\n  安装\nwget http://nginx.org/download/nginx-1.16.1.tar.gz tar -zxvf nginx-1.16.1.tar.gz cd nginx-1.16.1 sudo ./configure \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install whereis nginx # 查看nginx安装地址 /usr/local/nginx   BasicUse   启动\n/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf   重启\ncd /usr/local/nginx/sbin ./nginx -s reload   Example Conf # /usr/local/nginx/conf/nginx.conf  #user nobody; worker_processes 1; #error_log logs/error.log; #error_log logs/error.log notice; #error_log logs/error.log info;  #pid logs/nginx.pid;  events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; #log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39;  # \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39;  # \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;;  #access_log logs/access.log main;  sendfile on; #tcp_nopush on;  #keepalive_timeout 0;  keepalive_timeout 65; #gzip on;  # 包含aps的nginx配置  include /usr/local/nginx/conf/aps/*.conf; server { listen 80; server_name localhost; #charset koi8-r;  #access_log logs/host.access.log main;  location / { root html; index index.html index.htm; } #error_page 404 /404.html;  # redirect server error pages to the static page /50x.html  #  error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } # proxy the PHP scripts to Apache listening on 127.0.0.1:80  #  #location ~ \\.php$ {  # proxy_pass http://127.0.0.1;  #}  # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000  #  #location ~ \\.php$ {  # root html;  # fastcgi_pass 127.0.0.1:9000;  # fastcgi_index index.php;  # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name;  # include fastcgi_params;  #}  # deny access to .htaccess files, if Apache\u0026#39;s document root  # concurs with nginx\u0026#39;s one  #  #location ~ /\\.ht {  # deny all;  #}  } # another virtual host using mix of IP-, name-, and port-based configuration  #  #server {  # listen 8000;  # listen somename:8080;  # server_name somename alias another.alias;  # location / {  # root html;  # index index.html index.htm;  # }  #}  # HTTPS server  #  #server {  # listen 443 ssl;  # server_name localhost;  # ssl_certificate cert.pem;  # ssl_certificate_key cert.key;  # ssl_session_cache shared:SSL:1m;  # ssl_session_timeout 5m;  # ssl_ciphers HIGH:!aNULL:!MD5;  # ssl_prefer_server_ciphers on;  # location / {  # root html;  # index index.html index.htm;  # }  #} } # /usr/local/nginx/conf/aps/aps.conf server { listen 10050; server_name localhost; # 访问后端api  location /api/ { proxy_pass http://127.0.0.1:5500/; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } # 访问静态文件  location /static/ { alias /usr/local/nginx/html/aps/dist/; # 静态文件访问硬盘  } # 访问主页  location / { root /usr/local/nginx/html/aps/dist/; index index.html index.htm; } } ","date":"2019-08-26T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/nginx%E5%AE%89%E8%A3%85%E5%92%8C%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","title":"Nginx安装和基本使用"},{"content":"基本安装  使用rufus以dd模式写入U盘 从u盘启动 将manjaro启动项中的driver和boot添加或修改driver=intel才能进入安装界面(双显卡笔记本) 安装系统, 注意不要联网, 否则容易卡在安装 reboot进入系统 manjaro启动项中quiet后增加nouveau.modeset=0(双显卡)  双显卡使用prime管理连接外接显示器   删除bumblebee或者开源驱动\nsudo mhwd -r pci nonfree 0300   安装nvidia私有闭源驱动\n  方法一:\nsudo mhwd -i pci video-nvidia 或\nsudo mhwd -i pci video-nvidia-390xx # 390xx或者435xx, 数字是驱动版本...   方法二 系统设置-硬件设定中右键安装video-nvidia-390xx之类的驱动\n    安装依赖\nsudo pacman -S linuxXXX-headers acpi_call-dkms xorg-xrandr xf86-video-intel git  注: XXX 为内核版本， 本来我的5.3有点问题，降级成4.19才可以，以4.19为例便是 linux419-headers\n   注入\nsudo modprobe acpi_call   使用github上的脚本\ncd ~ # 建议在用户目录下操作 git clone https://github.com/dglt1/optimus-switch-sddm.git cd optimus-switch-sddm chmod +x install.sh sudo ./install.sh   reboot\n  ","date":"2019-08-01T22:21:57+08:00","permalink":"https://huangkai1008.github.io/p/manjaro%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","title":"Manjaro安装配置"},{"content":"Git提交代码时需要提交Message, 为了使得提交信息更清晰明了, 需要确定规范\n现在比较流行的规范是Angular规范, 也根据此规范衍生了Conventional Commits specification\n规范 格式  \u0026lt;type\u0026gt;(\u0026lt;scope\u0026gt;): \u0026lt;subject\u0026gt; \u0026lt;BLANK LINE\u0026gt; \u0026lt;body\u0026gt; \u0026lt;BLANK LINE\u0026gt; \u0026lt;footer\u0026gt; 按照空行分割为三个部分, 分别为Header，Body 和 Footer 其中，Header 是必需的，Body 和 Footer 可以省略 不管是哪一个部分，任何一行都不得超过72个字符（或100个字符）, 这是为了避免自动换行影响美观\n组成 Header Header部分只有一行，包括三个字段：type（必需）、scope（可选）和subject（必需）\n  type\n​type用于说明 commit 的类别，只允许使用下面7个标识\n  feat：新功能（feature） fix：修补bug docs：文档（documentation） style： 格式（不影响代码运行的变动） refactor：重构（即不是新增功能，也不是修改bug的代码变动） test：增加测试 chore：构建过程或辅助工具的变动   如果type为feat和fix，则该 commit 将肯定出现在 Change log 之中。其他情况（docs、chore、style、refactor、test）由你决定，要不要放入 Change log，建议是不要\n  scope\nscope用于说明 commit 影响的范围，比如数据层、控制层、视图层等等，视项目不同而不同\n  subject\nsubject是 commit 目的的简短描述，不超过50个字符\n  以动词开头，使用第一人称现在时，比如change，而不是changed或changes 第一个字母小写 结尾不加句号     Body Body 部分是对本次 commit 的详细描述，可以分成多行\n 使用第一人称现在时，比如使用change而不是changed或changes\n  应该说明代码变动的动机，以及与以前行为的对比\n Footer Footer部分可以用于表达不兼容变动和关闭Issue\n  不兼容变动\n如果当前代码与上一个版本不兼容，则 Footer 部分以BREAKING CHANGE开头，后面是对变动的描述、以及变动理由和迁移方法\n  关闭Issue\n Closes APS-151\n   Jetbrains工具配置   git commit template\n 提交信息模板\n   Gitmoji\n 添加emoji表情在commit信息中\n   ","date":"2019-07-12T14:14:15+08:00","permalink":"https://huangkai1008.github.io/p/git-commit-message%E7%BC%96%E5%86%99%E8%A7%84%E8%8C%83/","title":"Git Commit Message编写规范"},{"content":"Web Frameworks  Uvicorn 基于asyncio开发的一个轻量级高效的 web 服务器框架 Starlette Quart Responder Fastapi Sanic  Utils  Poetry 新的Python依赖包管理工具 Pipenv 用了很久的现在也在用的\u0026hellip; 有时候Locking速度感人, pipfile声明版本可以防止很多坑 Black 代码格式化库 Loguru python日志库 PySnooper python Debugger  ORM  Gino tortoise-orm  Test  locust 压力测试工具  Environment  python-dotenv environs  ","date":"2019-05-27T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/%E5%80%BC%E5%BE%97%E5%85%B3%E6%B3%A8%E7%9A%84python%E5%BA%93/","title":"值得关注的Python库"},{"content":"使用IDEA初始化Spring Boot项目   选择File -\u0026gt; New -\u0026gt; Project 新建项目\n  选择Spring Initializr， 点击Next，填写项目基本信息   项目依赖勾选Spring Web选择Finish等待项目构建\n  ​\n","date":"2019-02-07T11:15:10+08:00","permalink":"https://huangkai1008.github.io/p/spring%E5%88%9D%E5%A7%8B%E5%8C%96%E5%B7%A5%E5%85%B7/","title":"Spring初始化工具"},{"content":"What is wsgi **CGI(通用网关接口， Common Gateway Interface/CGI)** **CGI是定义客户端与web服务器交流方式的程序**。\u0026lt;u\u0026gt;例如正常情况下客户端发来一个请求，根据HTTP协议Web服务器将请求内容解析出来，进过计算后，再将加us安出来的内容封装好，例如服务器返回一个HTML页面，并且根据HTTP协议构建返回内容的响应格式。涉及到TCP连接、HTTP原始请求和相应格式的这些，都由一个软件来完成，这时，以上的工作需要一个程序来完成，而这个程序便是CGI\u0026lt;/u\u0026gt;** **WSGI(Web服务器网关接口(Python Web Server Gateway Interface，WSGI)** `WSGI`就是基于`Python`的以`CGI`为标准做一些扩展的协议  What is uwsgi uWSGI，是指实现了WSGI协议的一个web服务器。即用来接受客户端请求，转发响应的程序\nWhat is asgi 异步网关协议接口，一个介于网络协议服务和`Python`应用之间的标准接口，能够处理多种通用的协议类型，包括`HTTP`，`HTTP2`和`WebSocket`  ASGI尝试保持在一个简单的应用接口的前提下，提供允许数据能够在任意的时候、被任意应用进程发送和接受的抽象。并且同样描述了一个新的，兼容HTTP请求响应以及WebSocket数据帧的序列格式。允许这些协议能通过网络或本地socket进行传输，以及让不同的协议被分配到不同的进程中\nDifference between wsgi \u0026amp; asgi 1. Wsgi is based on `Http`, not support `websocket` 2. Asgi is the extension of wsgi.  ","date":"2019-01-24T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/wsgi-asgi-uwsgi%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"WSGI ASGI UWSGI的区别"},{"content":"RabbitMQ  Platform: Centos7  安装   install Erlang\nyum install erlang   install rabbitMQ\n# rpm安装 wget https://github.com/rabbitmq/rabbitmq-server/releases/download/rabbitmq_v3_6_14/rabbitmq-server-3.6.14-1.el7.noarch.rpm yum install rabbitmq-server-3.6.14-1.el7.noarch.rpm # yum安装 yum install rabbitmq-server   配置   启动远程访问\n[{rabbit, [ {loopback_users, []} ]}]   安装插件\n/sbin/rabbitmq-plugins enable rabbitmq_management   使用   服务命令\nsystemctl start rabbitmq-server.service # 启动 systemctl status rabbitmq-server.service\t# 查看状态 systemctl restart rabbitmq-server.service\t# 重启 systemctl enable rabbitmq-server.service # 开机自启   添加用户\nrabbitmqctl add_user root root123 # 添加新用户，用户名为 \u0026#34;root\u0026#34; ，密码为 \u0026#34;root123\u0026#34; rabbitmqctl set_permissions -p / root \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; # 为root用户添加所有权限 rabbitmqctl set_user_tags root administrator # 设置root 用户为管理员角色   访问web页面\nhttp://ip:15672   ","date":"2018-11-12T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/rabbitmq%E5%9F%BA%E7%A1%80%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8/","title":"RabbitMQ基础安装使用"},{"content":"mysql中min和max查询优化  max()函数需扫描where条件过滤后的所有行\n  慎用max()函数，特别是频繁执行的sql，若需用到可转化为order by id desc limit 1\n ","date":"2018-11-06T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/mysql%E4%BC%98%E5%8C%96/","title":"Mysql优化"},{"content":"Redis 安装   Platform: centos7\n  version: 5.0\n  安装\nwget http://download.redis.io/releases/redis-5.0.0.tar.gz # 获取包 tar -zxvf redis-5.0.0.tar.gz mv redis-5.0.0 /usr/local/redis make \u0026amp;\u0026amp; make install   Redis配置   设置配置文件目录\nmkdir -p /etc/redis cp redis.conf /etc/redis   修改配置文件\nvim /etc/redis/redis.conf daemonize yes (no -\u0026gt; yes) # 守护进程 bind 0.0.0.0 (127.0.0.1 -\u0026gt; 0.0.0.0) # 远程登录 protected-mode no (yes -\u0026gt; no) # 关闭保护模式/或者添加密码   Redis使用   启动\n/usr/local/bin/redis-server /etc/redis/redis.conf   查看启动\nps -ef | grep redis   客户端使用\nredis-cli # 进入 127.0.0.1:6379\u0026gt;set name Huang Ok redis-cli shutdown # 关闭客户端   开机启动配置\n# 开机启动要配置在 rc.local 中，而 /etc/profile 文件，要有用户登录了，才会被执行。 echo \u0026#34;/usr/local/bin/redis-server /etc/redis/redis.conf \u0026amp;\u0026#34; \u0026gt;\u0026gt; /etc/rc.local   Supervisor管理Redis   更改redis配置\nvim /etc/redis/redis.conf daemonize no (yes -\u0026gt; no) # 取消守护进程   创建supervisor对redis的配置文件\nvim /etc/supervisord.d/redis.ini  `redis.ini`文件如下  [program:redis] command=redis-server /etc/redis/redis.conf\t#\t启动Redis的命令 autostart=true\t#\tsupervisord启动时，该程序也启动 autorestart=true # 异常退出时，自动启动 startsecs=3\t# 启动后持续3s后未发生异常，才表示启动成功\t stdout_logfile=/var/log/supervisor/redis/redis.log # 标准输出流日志 stderr_logfile=/var/log/supervisor/redis/redis_err.log\t# 标准错误输出流日志   ","date":"2018-10-11T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/redis%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","title":"Redis安装配置"},{"content":"安装Angular 确保node/npm已安装 node -v 查看node版本 npm -v 查看npm版本 安装typescript npm install -g typescript 安装Angular CLI npm install -g @angular/cli ng version # 验证angular-cli版本 建立一个新的Angular项目 Angular CLI 为我们提供了两种方式，用于创建新的应用程序：\n  ng init - 在当前目录创建新的应用程序\n  ng new - 创建新的目录，然后在新建的目录中运行 ng init 命令\n因此 ng new 与 ng init 的功能是相似的，只是 ng new 会为我们创建新的目录\n  创建应用 ng new my-app 可用选项  --dry-run: boolean, 默认为 false, 若设置 dry-run 则不会创建任何文件 --verbose: boolean, 默认为 false --link-cli: boolean, 默认为 false, 自动链接到 @angular/cli 包 --skip-install: boolean, 默认为 false, 表示跳过 npm install --skip-git: boolean, 默认为 false, 表示该目录不初始化为 git 仓库 --skip-tests: boolean, 默认为 false, 表示不创建 tests 相关文件 --skip-commit: boolean, 默认为 false, 表示不进行初始提交 --directory: string, 用于设置创建的目录名，默认与应用程序的同名 --source-dir: string, 默认为 'src', 用于设置源文件目录的名称 --style: string, 默认为 'css', 用于设置选用的样式语法 ('css', 'less' or 'scss') --prefix: string, 默认为 'app', 用于设置创建新组件时，组件选择器使用的前缀 --mobile: boolean, 默认为 false,表示是否生成 Progressive Web App 应用程序 --routing: boolean, 默认为 false, 表示新增带有路由信息的模块，并添加到根模块中 --inline-style: boolean, 默认为 false, 表示当创建新的应用程序时，使用内联样式 --inline-template: boolean, 默认为 false, 表示当创建新的应用程序时，使用内联模板  ","date":"2018-10-09T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/angular-starter/","title":"Angular Starter"},{"content":"安装  Platform: centos7  ","date":"2018-09-12T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/mongodb%E7%9A%84%E5%AE%89%E8%A3%85/","title":"MongoDB的安装"},{"content":"MariaDB安装   platform: Centos7\n  Install\nyum install -y mariadb-server   MariaDB配置使用   Using\nsystemctl start mariadb.service # 启动 systemctl enable mariadb.service # 开机自启   Configure\n  首先是设置密码，会提示先输入密码\n Enter current password for root (enter for none): \u0026lt;–直接回车\n  Set root password? [Y/n] \u0026lt;– 是否设置root用户密码，输入y并回车或直接回车\n  New password: \u0026lt;– 设置root用户的密码\n  Re-enter new password: \u0026lt;– 再输入一次你设置的密码\n  其他配置\n  Remove anonymous users? [Y/n] \u0026lt;– 是否删除匿名用户，Y回车\n  Disallow root login remotely? [Y/n] \u0026lt;–是否禁止root远程登录, N回车,\n  Remove test database and access to it? [Y/n] \u0026lt;– 是否删除test数据库，Y回车\n  Reload privilege tables now? [Y/n] \u0026lt;– 是否重新加载权限表，Y回车2.开启远程访问\n   开启远程访问\n  GRANTALLPRIVILEGESON*.*TO\u0026#39;root\u0026#39;@\u0026#39;%\u0026#39;IDENTIFIEDBY\u0026#39;123456\u0026#39;WITHGRANTOPTION; 刷新权限  flushprivileges 配置文件地址     ","date":"2018-09-09T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/mariadb%E5%AE%89%E8%A3%85/","title":"MariaDB安装"},{"content":"Mysql安装  版本: 8.0 添加源 yum local install https://repo.mysql.com//mysql80-community-release-el7-1.noarch.rpm  安装 yum install mysql-community-server   Mysql配置   初始化\nsudo mysqld --initialize --user=mysql --basedir=/usr --datadir=/var/lib/mysql   启动mysql\nsystemctl start mysqld   设置mysql开机自启\nsystemctl enable mysqld   查看初始密码\ngrep \u0026#39;temporary password\u0026#39; /var/log/mysqld.log   进入mysql\nmysql -u root -p   修改密码\nALTERUSER\u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39;IDENTIFIEDBY\u0026#39;Huang|12345\u0026#39;  查看版本\nselectversion();+-----------+|version()|+-----------+|8.0.16|+-----------+1rowinset(0.00sec)  查看端口\nshowglobalvariableslike\u0026#39;port\u0026#39;;+---------------+-------+|Variable_name|Value|+---------------+-------+|port|3306|+---------------+-------+1rowinset(0.04sec)  远程访问\nusemysql;updateusersethost=\u0026#39;%\u0026#39;whereuser=\u0026#39;root\u0026#39;;flushprivileges  ","date":"2018-09-09T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/mysql%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","title":"Mysql安装配置"},{"content":"数据库系统的目的(Purpose of Database Systems) 在早期，数据库应用程序直接建立在文件系统之上，导致一系列的问题\n  数据冗余和不一致(Data redundancy and inconsistency)\n  数据访问难度大(Difficulty in accessing data)\n  数据隔离(Data isolation)\n  完整性问题(Integrity problems)\n 完整性约束(Integrity constraints)问题\n难以添加新约束和修改约束\n   原子性更新(Atomicity of updates)\n 更新失败可能会导致数据库的数据处于不一致的状态，或者只更新了部分数据\n例如: 从一方转账给另一方，只会有完成转账和完全没发生转账两种情况，不会出现转账方转账了但是收款方未收到款项的问题\n   多用户并发访问(Concurrent access by multiple users)\n 并发访问需要高性能的支持， 而不受控制的并发访问可能会导致数据不一致\n   安全问题(Security problems)\n 文件系统难以提供安全保障\n 数据库系统就是为了解决这些问题产生的\n  数据模型(Data Models) 组成  一系列用于描述的工具    数据(Data)\n  数据关系(Data relationships)\n 数据语义(Data semantics) 数据约束(Data constraints)     关系模型(Relational model) 实体关系数据模型(Entity-Relationship data model 主要用于数据库设计) 基于对象的数据模型(Object-based data models (Object-oriented and Object-relational)) 半结构化数据模型(Semi-structured data model (XML))  数据视图(View of Data) 一个数据库系统的结构如下图 模式与实例(Instances and Schema) 类似于编程语言中的类型和变量\n 逻辑模式(logic schema) 数据库的总体逻辑结构，类似于程序设计中的变量类型信息 物理模式(physical schema) 数据库的总体物理结构 实例(instance) 数据库在特定时间点的实际内容， 类似于变量的值  物理数据独立性(Physical Data Independence)  定义： 在不更改逻辑模式的情况下修改物理模式的能力\n   应用程序依赖于逻辑模式(logic schema)\n  一般来说，不同级别和组件应该定义得很好，以便在某些部分中进行更改，不严重影响他人\n  数据定义语言(Data Definition Language)  定义数据库模式的规范表示法\n createtableinstructor(IDchar(5),namevarchar(20),dept_namevarchar(20),salarynumeric(8,2))DDL编译器生成一组存储表模板信息的数据字典（data dictionary)\n数据字典包含元信息(metadata)\n 数据库模式(database schema) 完整性约束(Integrity constraints)  主键   授权(Authorization)  数据处理语言(Data Manipulation Language )  用于访问和更新由适当数据模型组织的数据的语言（查询语言）\n  DML一般分为两种类型  Pure Commercial  例如SQL      ​\n结构化查询语言(Structured Query Language, SQL)  SQL查询语言是非过程的查询将多个表（可能只有一个）作为输入，并始终返回一个表(SQL query language is nonprocedural. A query takes as input several tables (possibly only one) and always returns a single table)\n 数据库设计(Database Design)   逻辑设计(logic design) \u0026ndash; 决定数据库模式\n  业务决定\n 我们应该在数据库中记录哪些属性\n   计算机科学决定\n  我们应该有什么关系模式     属性应该如何分布在不同的关系模式中       物理设计(physical design) \u0026ndash; 决定数据库的物理布局\n  ","date":"2018-09-09T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%AE%BA-%E4%BB%8B%E7%BB%8D/","title":"数据库系统概论-介绍"},{"content":"安装   Platform: Centos7\n  version: 1.12\n  安装\ncd /opt wget https://studygolang.com/dl/golang/go1.12.4.linux-amd64.tar.gz tar xzvf go1.12.4.linux-amd64.tar.gz\t# 安装   配置环境变量\nvim ~/.zshrc\t# 如果用bash就是vim ~/.bashrc # 追加golang配置 export GOROOT=/opt/go export PATH=$PATH:$GOROOT/bin # 立即生效 source ~/.zshrc # 查看版本 go version   ","date":"2018-07-31T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/golang%E5%AE%89%E8%A3%85/","title":"Golang安装"},{"content":"Python默认的json模块序列化并不是很全面，只能序列化基本的数据类型, 像一些时间格式或者自定义类型都不能序列化，所以在有些时候需要扩展json模块的json encoder\n扩展 import datetime as dt import decimal import json import enum from collections.abc import Iterator class ExtendedEncoder(json.JSONEncoder): def default(self, o): if isinstance(o, dt.datetime): return o.strftime(\u0026#39;%Y-%m-%d%H:%M:%S\u0026#39;) elif isinstance(o, dt.date): return o.strftime(\u0026#39;%Y-%m-%d\u0026#39;) elif isinstance(o, decimal.Decimal): return float(o) elif isinstance(o, Iterator): return list(o) elif isinstance(o, enum.Enum): return o.value return json.JSONEncoder.default(self, o) 使用场景   日常格式化\n例如对于日期格式的格式化\nimport datetime as dt now = dt.datetime.now() 对于now如果使用json.dumps(t_now)便会触发TypeError: Object of type datetime is not JSON serializable使用扩展的Encoder\n\u0026gt;\u0026gt;\u0026gt; json.dumps(now, cls=ExtendedEncoder) \u0026#39;2018-04-09 23:04:49\u0026#39;   Flask\n修改flask类的json_encoder\nfrom flask import Flask as _Flask class QuizFlask(_Flask): \u0026#34;\u0026#34;\u0026#34; 自定义flask \u0026#34;\u0026#34;\u0026#34; json_encoder = ExtendedEncoder def make_response(self, rv): if rv is None: rv = dict() if isinstance(rv, Iterator): rv = list(rv) return super(QuizFlask, self).make_response(rv)   Tortoise-orm\n模型jsonfield的encoder\nimport json from tortoise import fields from tortoise.models import Model __all__ = [\u0026#39;OurModel\u0026#39;] class OurModel(Model): \u0026#34;\u0026#34;\u0026#34;示例model\u0026#34;\u0026#34;\u0026#34; id = fields.IntField(pk=True) cat_ids = fields.JSONField( encoder=ExtendedEncoder, decoder=json.decoder ) # JsonField的encoder   ","date":"2018-04-09T22:49:14+08:00","permalink":"https://huangkai1008.github.io/p/%E6%89%A9%E5%B1%95python-json-encoder/","title":"扩展Python Json Encoder"},{"content":"介绍 从Python3.2开始，标准库为我们提供了 concurrent.futures 模块，它提供了 ThreadPoolExecutor (线程池)和ProcessPoolExecutor (进程池)两个类。\n相比 threading 等模块，该模块通过 submit 返回的是一个 future 对象，它是一个未来可期的对象，通过它可以获悉线程的状态主线程(或进程)中可以获取某一个线程(进程)执行的状态或者某一个任务执行的状态及返回值：\n1.主线程可以获取某一个线程（或者任务的）的状态，以及返回值。\n2.当一个线程完成的时候，主线程能够立即知道。\n3.让多线程和多进程的编码接口一致。\n基本使用 from concurrent.futures import ThreadPoolExecutor import time def get_page(url): time.sleep(url) return url with ThreadPoolExecutor(max_workers=5) as t: # 创建一个最大容纳数量为5的线程池 task1 = t.submit(get_page, 1) task2 = t.submit(get_page, 2) # 通过submit提交执行的函数到线程池中 task3 = t.submit(get_page, 3) print(f\u0026#34;task1: {task1.done()}\u0026#34;) # 通过done来判断线程是否完成 print(f\u0026#34;task2: {task2.done()}\u0026#34;) print(f\u0026#34;task3: {task3.done()}\u0026#34;) time.sleep(2.5) print(f\u0026#34;task1: {task1.done()}\u0026#34;) print(f\u0026#34;task2: {task2.done()}\u0026#34;) print(f\u0026#34;task3: {task3.done()}\u0026#34;) print(task1.result()) # 通过result来获取返回值 \u0026gt;\u0026gt;\u0026gt; task1: False task2: False task3: False ... task1: True task2: True task3: False Api as_completed  concurrent.futures.as_completed(fs, timeout=None)\n  返回一个生成器在迭代过程中会阻塞\n  直到线程完成或者异常时,返回一个被set_result的Future对象\n  此方法的返回顺序为哪个线程先失败/完成就返回\n from concurrent.futures import ThreadPoolExecutor, as_completed import time def get_page(url): time.sleep(url) return url with ThreadPoolExecutor(max_workers=5) as t: # 创建一个最大容纳数量为5的线程池 tasks = [t.submit(get_page, page) for page in range(1, 5)] for future in as_completed(tasks): result = future.result() print(result) \u0026gt;\u0026gt;\u0026gt; 1 2 3 4 wait  concurrent.futures.wait(fs, timeout=None, return_when=ALL_COMPLETED)\n  fs: 执行的序列\n  timeout: 等待的最大时间，如果超过这个时间即使线程未执行完成也将返回\n  return_when: 表示wait返回结果的条件，默认为 ALL_COMPLETED 全部执行完成再返回\n   FIRST_COMPLETED   函数将在任意可等待对象结束或取消时返回。    FIRST_EXCEPTION   函数将在任意可等待对象因引发异常而结束时返回。 当没有引发任何异常时它就相当于 ALL_COMPLETED。    ALL_COMPLETED   函数将在所有可等待对象结束或取消时返回。  from concurrent.futures import ThreadPoolExecutor, wait import time def get_page(url): time.sleep(url) return url with ThreadPoolExecutor(max_workers=5) as t: # 创建一个最大容纳数量为5的线程池 tasks = [t.submit(get_page, page) for page in range(1, 5)] a, b = wait(tasks) print(a) print(b) \u0026gt;\u0026gt;\u0026gt; {\u0026lt;Future at 0x1c071fb1f28 state=finished returned int\u0026gt;, \u0026lt;Future at 0x1c071fb1d68 state=finished returned int\u0026gt;, \u0026lt;Future at 0x1c071f9fd68 state=finished returned int\u0026gt;, \u0026lt;Future at 0x1c071d78278 state=finished returned int\u0026gt;} set() map  *concurrent.futures.Executor.map(fn, iterables, timeout=None)\n  fn: 第一个参数 fn 是需要线程执行的函数\n  *iterables: 第二个参数接受一个可迭代对象\n  timeout: 第三个参数 timeout 跟 wait() 的 timeout 一样，但由于 map 是返回线程执行的结果，如果 timeout小于线程执行时间会抛异常 TimeoutError\n from concurrent.futures import ThreadPoolExecutor import time def get_page(url): time.sleep(url) return url URLS = [url for url in range(1, 4)] with ThreadPoolExecutor(max_workers=5) as executor: # 创建一个最大容纳数量为5的线程池 for result in executor.map(get_page, URLS): print(result) \u0026gt;\u0026gt;\u0026gt; 1 2 3 回调函数 回调函数(add_done_callback)是在调用线程完成后再调用的\nfrom concurrent.futures import ThreadPoolExecutor, wait import threading import time def get_page(url): time.sleep(url) return url def call_back(worker): print(f\u0026#39;tid: {threading.current_thread().ident}\u0026#39;, worker.result()) with ThreadPoolExecutor() as t: tasks = [] for page in range(1, 5): task = t.submit(get_page, url=page) task.add_done_callback(call_back) tasks.append(task) wait(tasks) \u0026gt;\u0026gt;\u0026gt; tid: 6392 1 tid: 14936 2 tid: 12516 3 tid: 10524 4 异常处理  通过添加回调函数的方法处理异常  import logging def executor_callback(worker): logging.info(f\u0026#39;finished\u0026#39;) worker_exception = worker.exception() if worker_exception: logging.exception(worker_exception) 备注  一定使用with关键字处理线程池，在某些情况下线程池可能不能自动回收线程资源，with可以避免内存持续增长等情况  ","date":"2018-02-08T22:49:14+08:00","permalink":"https://huangkai1008.github.io/p/python%E7%BA%BF%E7%A8%8B%E6%B1%A0%E4%BD%BF%E7%94%A8/","title":"Python线程池使用"},{"content":"  Platform: centos7\n  version: 5.0\n  安装   Uninstall old versions\nsudo yum remove docker \\  docker-client \\  docker-client-latest \\  docker-common \\  docker-latest \\  docker-latest-logrotate \\  docker-logrotate \\  docker-engine   Install Docker CE\nsudo yum install -y yum-utils \\  device-mapper-persistent-data \\  lvm2 # 设置stable源 sudo yum-config-manager \\  --add-repo \\  https://download.docker.com/linux/centos/docker-ce.repo # 安装Docker CE sudo yum install docker-ce docker-ce-cli containerd.io   启动  Docker启动 sudo systemctl start docker\t# 启动Docker sudo systemctl status docker\t# 查看Docker状态   ","date":"2018-01-31T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/docker%E5%AE%89%E8%A3%85/","title":"Docker安装"},{"content":"  遇到in查询之类的批量删除或者更新，可以使用synchronize_session=False\ndb.session.delete(synchronize_session=False)   使用find_in_set\nfrom sqlalchemy.sql.expression import func db.session.query(Post).filter(func.find_in_set(\u0026#39;10\u0026#39;, Post.c.tag_id))   批量增加删除\ndb.session.add_all(instances) db.session.delete_all(instances)   Mysql IS NULL判断\nisnot() is_()   Mysql 联合主键\nfrom sqlalchemy import PrimaryKeyConstraint class Node(Model): __table_args__ = ( PrimaryKeyConstraint(\u0026#39;pk1\u0026#39;, \u0026#39;pk2\u0026#39;), )   Flask_sqlalchemy支持Double精度类型字段\nfrom sqlalchemy import Column from sqlalchemy.dialects.mysql import DOUBLE from app import db class BaseModel(Model): id = db.Column(db.Integer, primary_key=True) # Flask_sqlalchemy double_column = Column(DOUBLE, comment=\u0026#39;双精度字段\u0026#39;) # Sqlalchemy mysql double column   subquery使用实例\nconditions = list() for key, value in material_period.items(): condition = and_( CraftEntityAttrs.attr_number == key, CraftEntityAttrs.attr_value == value ) conditions.append(condition) if not conditions: return list() stmt = ( db.session.query(CraftEntityAttrs.entity_id, CraftEntityAttrs.cat_number) .filter(or_(*conditions)) .subquery() ) query = db.session.query( CraftEntityPeriodHours.proc_number, CraftEntityPeriodHours.period, CraftEntityPeriodHours.hours, CraftEntityPeriodHours.major_wrapper_skill_level, stmt.c.cat_number, ).filter(CraftEntityPeriodHours.entity_id == stmt.c.entity_id) stmt = ( db.session.query(ProducePlan.row_id, ProducePlan.row_seq) .filter(ProducePlan.proc_number.in_(constants.COIL_PROC_NUMBERS)) .distinct() .subquery() ) query = ( BatchDetail.query.join( stmt, and_( BatchDetail.row_id == stmt.c.row_id, BatchDetail.row_seq == stmt.c.row_seq, ), ) .join(PlanRow, BatchDetail.row_id == PlanRow.id) .join(RowProject, PlanRow.project_id == RowProject.id) .join(Order, RowProject.order_id == Order.id) .with_entities( Order.order_number, Order.id.label(\u0026#39;order_id\u0026#39;), Order.project_name, RowProject.row_project_number, RowProject.id.label(\u0026#39;project_id\u0026#39;), Order.purchase_unit, RowProject.fac_number, RowProject.mat_number, RowProject.mat_desc, PlanRow.com_qty, BatchDetail.row_id, PlanRow.plan_row_number, BatchDetail.batch_id, BatchDetail.batch_number, BatchDetail.batch_qty, BatchDetail.batch_seq, BatchDetail.single_pack_cycle, ) .order_by(RowProject.id, BatchDetail.batch_id, BatchDetail.batch_seq) ) stmt = ( db.session.query(ProducePlan.project_id) .outerjoin( ProducePlanCompletion, ProducePlan.plan_id == ProducePlanCompletion.plan_id ) .filter( or_( ProducePlanCompletion.completion.is_(None), ProducePlanCompletion.completion == constants.ProducePlanCompletion.not_scheduled.value, ) ) .distinct() .subquery() ) query = db.session.query(ProducePlan.project_id).filter( ProduceUserPlan.project_id.in_(stmt), ProduceUserPlan.proc_type == \u0026#39;design\u0026#39; )   ","date":"2017-06-14T13:56:20+08:00","permalink":"https://huangkai1008.github.io/p/sqlalchemy%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F/","title":"Sqlalchemy使用注意"},{"content":"Python安装   Platform: centos7\n  Version: 3.7\n  安装编译环境\nyum install zlib-devel bzip2-devel openssl-devel ncurses-devel libffi-devel   下载\nwget --no-check-certificate https://www.python.org/ftp/python/3.7.4/Python-3.7.4.tgz   创建安装目录解压\nsudo mkdir /usr/local/python3 tar -zxvf Python-3.7.4.tgz cd Python-3.7.4/   编译安装\nsudo ./configure --prefix=/usr/local/python3 # 指定创建的目录 make \u0026amp;\u0026amp; make install # 编译安装   软链接  创建python和pip软链接 ln -s /usr/local/python3/bin/python3 /usr/bin/python3 # python3 软链接 ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3 # pip3软链接 ln -s /usr/local/python3/bin/pipenv /usr/bin/pipenv # pipenv软链接   使用pyenv管理多个Python版本 安装pyenv  安装脚本 curl https://pyenv.run | bash  manjaro sudo pacman -S pyenv   pyenv基本使用  展示可以安装的版本 pyenv install --list  安装python pyenv install 3.7.4  manjaro如遇到ModuleNotFoundError: No module named \u0026lsquo;_ctypes\u0026rsquo;, 可执行sudo pacman -S pkgconf libffi\n  查看可使用的版本，带*表示当前使用的版本 $ pyenv versions * system (set by /home/huangkai/.pyenv/version) 3.7.4  配置及管理python版本  使用pyenv global 配置当前用户的系统使用的python版本 使用pyenv shell 配置当前shell的python版本，退出shell则失效 使用pyenv local 配置所在项目（目录）的python版本    ","date":"2017-01-08T22:49:14+08:00","permalink":"https://huangkai1008.github.io/p/python%E5%AE%89%E8%A3%85/","title":"Python安装"}]